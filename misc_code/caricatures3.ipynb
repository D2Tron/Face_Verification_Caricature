{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torchvision.io import read_image\n",
    "from sklearn.metrics import multilabel_confusion_matrix, f1_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "import math\n",
    "import random\n",
    "import wandb\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Variables\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\") # Use GPU or CPU for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f66c267f730>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "    name=\"sub rand car new_dl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlteredNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlteredNet, self).__init__()\n",
    "\n",
    "        # self.softmax_layer = nn.Softmax(dim=1)\n",
    "        self.sigmoid_layer = nn.Sigmoid()\n",
    "\n",
    "        self.model = torchvision.models.resnet18(\n",
    "            weights=torchvision.models.ResNet18_Weights.DEFAULT\n",
    "        )\n",
    "        self.model.fc = nn.Linear(in_features=self.model.fc.in_features, out_features=150)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HierarchicalNet, self).__init__()\n",
    "\n",
    "        # Shared backbone network (e.g., ResNet18)\n",
    "        self.backbone = torchvision.models.resnet18(pretrained=True)\n",
    "        num_features = self.backbone.fc.in_features\n",
    "\n",
    "        # Level 1 prediction (groups)\n",
    "        self.prominent_prediction = nn.Linear(num_features, 18)  # 18 prominent features\n",
    "\n",
    "        # Level 2 prediction (subfeatures within groups) (150 total)\n",
    "        self.cheekbones_subfeature_prediction = nn.Linear(num_features, 2)  # Two subfeatures for cheekbones\n",
    "        self.cheeks_subfeature_prediction = nn.Linear(num_features, 3)  # Three subfeatures for cheeks\n",
    "        self.chin_subfeature_prediction = nn.Linear(num_features, 10)  # Ten subfeatures for chin\n",
    "        self.ears_subfeature_prediction = nn.Linear(num_features, 8)  # Eight subfeatures for ears\n",
    "        self.eyebrows_subfeature_prediction = nn.Linear(num_features, 14)  # Fourteen subfeatures for eyebrows\n",
    "        self.eyelids_subfeature_prediction = nn.Linear(num_features, 4)  # Four subfeatures for eyelids\n",
    "        self.eyes_subfeature_prediction = nn.Linear(num_features, 18)  # Sixteen subfeatures for eyes\n",
    "        self.facialhair_subfeature_prediction = nn.Linear(num_features, 11)  # Eleven subfeatures for facial hair\n",
    "        self.forehead_subfeature_prediction = nn.Linear(num_features, 6)  # Six subfeatures for forehead\n",
    "        self.hair_subfeature_prediction = nn.Linear(num_features, 16)  # Sixteen subfeatures for hair\n",
    "        self.head_subfeature_prediction = nn.Linear(num_features, 6)  # Six subfeatures for head\n",
    "        self.lips_subfeature_prediction = nn.Linear(num_features, 8)  # Seven subfeatures for lips\n",
    "        self.mouth_subfeature_prediction = nn.Linear(num_features, 3)  # Three subfeatures for mouth\n",
    "        self.neck_subfeature_prediction = nn.Linear(num_features, 4)  # Four subfeatures for neck\n",
    "        self.nose_subfeature_prediction = nn.Linear(num_features, 23)  # Twenty-One subfeatures for nose        \n",
    "        self.skin_subfeature_prediction = nn.Linear(num_features, 5)  # Five subfeatures for skin\n",
    "        self.teeth_subfeature_prediction = nn.Linear(num_features, 8)  # Eight subfeatures for teeth\n",
    "        self.upperlip_subfeature_prediction = nn.Linear(num_features, 1)  # One subfeatures for upper lip\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        # Level 1 prediction\n",
    "        prominent_output = self.prominent_prediction(features)\n",
    "\n",
    "        # Level 2 predictions within each group\n",
    "        cheekbones_output = self.cheekbones_subfeature_prediction(features)  # Predicts subfeatures for cheek bones\n",
    "        cheeks_output = self.cheeks_subfeature_prediction(features)  # Predicts subfeatures for cheeks\n",
    "        chin_output = self.chin_subfeature_prediction(features)  # Predicts subfeatures for chin\n",
    "        ears_output = self.ears_subfeature_prediction(features)  # Predicts subfeatures for ears\n",
    "        eyebrows_output = self.eyebrows_subfeature_prediction(features)  # Predicts subfeatures for eyebrows\n",
    "        eyelids_output = self.eyelids_subfeature_prediction(features)  # Predicts subfeatures for eyelids\n",
    "        eyes_output = self.eyes_subfeature_prediction(features)  # Predicts subfeatures for eyes\n",
    "        facialhair_output = self.facialhair_subfeature_prediction(features)  # Predicts subfeatures for facial hair\n",
    "        forehead_output = self.forehead_subfeature_prediction(features)  # Predicts subfeatures for forehead\n",
    "        hair_output = self.hair_subfeature_prediction(features)  # Predicts subfeatures for hair\n",
    "        head_output = self.head_subfeature_prediction(features)  # Predicts subfeatures for head\n",
    "        lips_output = self.lips_subfeature_prediction(features)  # Predicts subfeatures for lips\n",
    "        mouth_output = self.mouth_subfeature_prediction(features)  # Predicts subfeatures for mouth\n",
    "        neck_output = self.neck_subfeature_prediction(features)  # Predicts subfeatures for neck\n",
    "        nose_output = self.nose_subfeature_prediction(features)  # Predicts subfeatures for nose\n",
    "        skin_output = self.skin_subfeature_prediction(features)  # Predicts subfeatures for skin\n",
    "        teeth_output = self.teeth_subfeature_prediction(features)  # Predicts subfeatures for teeth\n",
    "        upperlip_output = self.upperlip_subfeature_prediction(features)  # Predicts subfeatures for upper lip\n",
    "\n",
    "        return prominent_output, cheekbones_output, cheeks_output, chin_output, ears_output, eyebrows_output, eyelids_output, eyes_output, facialhair_output, forehead_output, hair_output, head_output, lips_output, mouth_output, neck_output, nose_output, skin_output, teeth_output, upperlip_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" #   Create Dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((178, 218))\n",
    "])\n",
    "\n",
    "class CaricatureDataset(Dataset):\n",
    "    def __init__(self, labels_file, root_dir, transform=None):\n",
    "        self.labels = pd.read_csv(labels_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        class_index = math.floor(idx / 5)\n",
    "        self_car = self.labels.iloc[class_index, 0].strip() + \"_caricature\"\n",
    "        img_person = os.path.join(self.root_dir, self_car)\n",
    "        labels = self.labels.iloc[class_index, 1].replace('[', '').replace(']', '').split('.')[:-1]\n",
    "        labels = np.array([int(label) for label in labels])\n",
    "        \n",
    "        images = os.listdir(img_person)\n",
    "        selected_images = random.sample(images, 5)\n",
    "        \n",
    "        samples = []\n",
    "        for image_file in selected_images:\n",
    "            image_path = os.path.join(img_person, image_file)\n",
    "            img = read_image(image_path)\n",
    "            if img.shape[0] == 1:\n",
    "                img = np.repeat(img, 3, axis=0)\n",
    "            sample = img, labels\n",
    "            samples.append(sample)\n",
    "        \n",
    "        return samples[idx % 5] \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" class CaricatureDataset(Dataset):\n",
    "    def __init__(self, labels_file, root_dir, split, transform=None):\n",
    "        self.annotations = pd.read_csv(labels_file, names=[\"id\", \"label\"], sep=\",\")\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.identities = []\n",
    "        for id in self.annotations[\"id\"].values.tolist():\n",
    "            folder_name = id + \"_caricature\"\n",
    "\n",
    "            image_names = os.listdir(os.path.join(root_dir, folder_name))\n",
    "            random.shuffle(image_names)\n",
    "            for img_index, img in enumerate(image_names):\n",
    "                if split==\"Train\" and img_index<4:\n",
    "                    self.image_paths.append((os.path.join(root_dir, folder_name, img)))\n",
    "                    self.identities.append(id)\n",
    "                elif split==\"Test\" and img_index==4:\n",
    "                    self.image_paths.append((os.path.join(root_dir, folder_name, img)))\n",
    "                    self.identities.append(id)\n",
    "\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        id = self.identities[idx]\n",
    "\n",
    "        image = torchvision.io.read_image(image_path, mode=torchvision.io.ImageReadMode.RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.annotations[self.annotations[\"id\"]==id][\"label\"].values[0]\n",
    "\n",
    "        label = label.strip(\"[]\").split()\n",
    "\n",
    "        # Convert strings to float and create a list\n",
    "        label = [float(val) for val in label]\n",
    "\n",
    "        return image, torch.tensor(label) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaricatureDataset(Dataset):\n",
    "    def __init__(self, labels_file, root_dir, split, transform=None):\n",
    "        self.annotations = pd.read_csv(labels_file, index_col=0)\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "        self.image_paths = []\n",
    "        self.identities = self.annotations.axes[0].tolist()\n",
    "\n",
    "        for id in self.identities:\n",
    "            folder_name = id + \"_caricature\"\n",
    "\n",
    "            image_names = os.listdir(os.path.join(root_dir, folder_name))\n",
    "            random.shuffle(image_names)\n",
    "            for img_index, img in enumerate(image_names):\n",
    "                if split==\"Train\" and img_index<4:\n",
    "                    self.image_paths.append((os.path.join(root_dir, folder_name, img)))\n",
    "                elif split==\"Test\" and img_index==4:\n",
    "                    self.image_paths.append((os.path.join(root_dir, folder_name, img)))\n",
    "\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        # print(len(self.image_paths))\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        if split==\"Train\":\n",
    "            class_index = math.floor(idx/4)\n",
    "        elif split==\"Test\":\n",
    "            class_index = idx\n",
    "        id = self.identities[class_index]\n",
    "        image = torchvision.io.read_image(image_path, mode=torchvision.io.ImageReadMode.RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = self.annotations.loc[id]\n",
    "\n",
    "        return image, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Load the data\n",
    "#   Create Dataloader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((218 , 178))\n",
    "])\n",
    "\n",
    "train_dataset = CaricatureDataset(labels_file='binary_sub_labels.csv', root_dir='/home/jsutariya/Desktop/Project/ourcar/', split=\"Train\", transform=transform)\n",
    "test_dataset = CaricatureDataset(labels_file='binary_sub_labels.csv', root_dir='/home/jsutariya/Desktop/Project/ourcar/', split=\"Test\", transform=transform)\n",
    "\n",
    "#   Create the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the images\n",
    "# i=0\n",
    "# for batch in test_loader:\n",
    "#     images, labels, id = batch\n",
    "    \n",
    "#     # Display the images\n",
    "#     grid = torchvision.utils.make_grid(images, nrow=5)\n",
    "#     plt.imshow(grid.permute(1, 2, 0))\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "\n",
    "#     # Get the subject names\n",
    "#     # subject_names = [dataset.annotations.iloc[math.floor(i / 5), 0].strip() + \"_caricature\" for i in range(len(images))]\n",
    "#     print(\"Subject Names:\", id)\n",
    "#     labels_list = labels.tolist()\n",
    "#     for list in labels_list:\n",
    "#         print(list)\n",
    "    \n",
    "#     i+=1\n",
    "#     if i == 3:\n",
    "#         break  # Only process the first batch for demonstration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AlteredNet().to(device)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "def train_one_epoch(epoch_index, accp, totp):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        inputs, labels = inputs.to(device).float(), labels.to(device).float()\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        output = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(output, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "\n",
    "        output = model.sigmoid_layer(output)\n",
    "\n",
    "        predictions = np.round(output.detach().cpu().numpy()).flatten()\n",
    "        target = labels.detach().cpu().numpy().flatten()\n",
    "        for index, prediction in enumerate(predictions):\n",
    "            if prediction == target[index]:\n",
    "                accp += 1\n",
    "            totp += 1\n",
    "        acc = accp/totp\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return last_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 0.7376927852630615\n",
      "  batch 20 loss: 0.7033847332000732\n",
      "  batch 30 loss: 0.6597188472747803\n",
      "  batch 40 loss: 0.6117695689201355\n",
      "  batch 50 loss: 0.5698795974254608\n",
      "  batch 60 loss: 0.5446260452270508\n",
      "  batch 70 loss: 0.5134212911128998\n",
      "  batch 80 loss: 0.49137944877147677\n",
      "  batch 90 loss: 0.4757650136947632\n",
      "  batch 100 loss: 0.45795331001281736\n",
      "  batch 110 loss: 0.443063884973526\n",
      "  batch 120 loss: 0.42872370183467867\n",
      "  batch 130 loss: 0.41806496381759645\n",
      "  batch 140 loss: 0.4058567702770233\n",
      "  batch 150 loss: 0.39989835023880005\n",
      "  batch 160 loss: 0.38923689126968386\n",
      "  batch 170 loss: 0.37518560588359834\n",
      "  batch 180 loss: 0.3779471665620804\n",
      "  batch 190 loss: 0.3765044003725052\n",
      "  batch 200 loss: 0.36883204281330106\n",
      "LOSS train 0.36883204281330106 valid 0.36852043867111206\n",
      "ACC train 0.7993642769607843 valid 0.865234375\n",
      "F1 score: 0.8363733033162881\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 0.36421593725681306\n",
      "  batch 20 loss: 0.35951915085315705\n",
      "  batch 30 loss: 0.3559811353683472\n",
      "  batch 40 loss: 0.34202462434768677\n",
      "  batch 50 loss: 0.3398709028959274\n",
      "  batch 60 loss: 0.34469627141952514\n",
      "  batch 70 loss: 0.34263262450695037\n",
      "  batch 80 loss: 0.33279502391815186\n",
      "  batch 90 loss: 0.33586710691452026\n",
      "  batch 100 loss: 0.3349554479122162\n",
      "  batch 110 loss: 0.3324024796485901\n",
      "  batch 120 loss: 0.3433720856904984\n",
      "  batch 130 loss: 0.3288403511047363\n",
      "  batch 140 loss: 0.3427742928266525\n",
      "  batch 150 loss: 0.33044724762439726\n",
      "  batch 160 loss: 0.3280425935983658\n",
      "  batch 170 loss: 0.3202480137348175\n",
      "  batch 180 loss: 0.31150477230548856\n",
      "  batch 190 loss: 0.3195086807012558\n",
      "  batch 200 loss: 0.31836561262607577\n",
      "LOSS train 0.31836561262607577 valid 0.3235095739364624\n",
      "ACC train 0.8799881280637255 valid 0.8671875\n",
      "F1 score: 0.8361558787561146\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 0.3124693214893341\n",
      "  batch 20 loss: 0.32811887860298156\n",
      "  batch 30 loss: 0.31239221394062044\n",
      "  batch 40 loss: 0.3199192643165588\n",
      "  batch 50 loss: 0.3170634627342224\n",
      "  batch 60 loss: 0.3121244013309479\n",
      "  batch 70 loss: 0.31042452454566954\n",
      "  batch 80 loss: 0.3073528468608856\n",
      "  batch 90 loss: 0.3024285614490509\n",
      "  batch 100 loss: 0.3213770478963852\n",
      "  batch 110 loss: 0.30838650465011597\n",
      "  batch 120 loss: 0.3208465963602066\n",
      "  batch 130 loss: 0.3191086381673813\n",
      "  batch 140 loss: 0.3166051656007767\n",
      "  batch 150 loss: 0.3127849817276001\n",
      "  batch 160 loss: 0.31600041687488556\n",
      "  batch 170 loss: 0.30719689428806307\n",
      "  batch 180 loss: 0.310423743724823\n",
      "  batch 190 loss: 0.3205155998468399\n",
      "  batch 200 loss: 0.31154693067073824\n",
      "LOSS train 0.31154693067073824 valid 0.31195035576820374\n",
      "ACC train 0.8805625765931373 valid 0.873046875\n",
      "F1 score: 0.845858908921141\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 0.3001511961221695\n",
      "  batch 20 loss: 0.3096848875284195\n",
      "  batch 30 loss: 0.2981716752052307\n",
      "  batch 40 loss: 0.29920957386493685\n",
      "  batch 50 loss: 0.3089654535055161\n",
      "  batch 60 loss: 0.2967783212661743\n",
      "  batch 70 loss: 0.3143693596124649\n",
      "  batch 80 loss: 0.32272800505161287\n",
      "  batch 90 loss: 0.29776273369789125\n",
      "  batch 100 loss: 0.30405460596084594\n",
      "  batch 110 loss: 0.3157405614852905\n",
      "  batch 120 loss: 0.3097934156656265\n",
      "  batch 130 loss: 0.2962295234203339\n",
      "  batch 140 loss: 0.30330400466918944\n",
      "  batch 150 loss: 0.302947598695755\n",
      "  batch 160 loss: 0.297906294465065\n",
      "  batch 170 loss: 0.30645851194858553\n",
      "  batch 180 loss: 0.3101472020149231\n",
      "  batch 190 loss: 0.3094548016786575\n",
      "  batch 200 loss: 0.3109210878610611\n",
      "LOSS train 0.3109210878610611 valid 0.30549684166908264\n",
      "ACC train 0.8807157628676471 valid 0.87109375\n",
      "F1 score: 0.8426668258733624\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.3020064741373062\n",
      "  batch 20 loss: 0.288192081451416\n",
      "  batch 30 loss: 0.2888649746775627\n",
      "  batch 40 loss: 0.3065095037221909\n",
      "  batch 50 loss: 0.31221584379673006\n",
      "  batch 60 loss: 0.30038727521896363\n",
      "  batch 70 loss: 0.2866967409849167\n",
      "  batch 80 loss: 0.3009035259485245\n",
      "  batch 90 loss: 0.3033357381820679\n",
      "  batch 100 loss: 0.29573734998703005\n",
      "  batch 110 loss: 0.31110533475875857\n",
      "  batch 120 loss: 0.2992185980081558\n",
      "  batch 130 loss: 0.30959639251232146\n",
      "  batch 140 loss: 0.2925799176096916\n",
      "  batch 150 loss: 0.29558880627155304\n",
      "  batch 160 loss: 0.3093689262866974\n",
      "  batch 170 loss: 0.30735627114772796\n",
      "  batch 180 loss: 0.29737711846828463\n",
      "  batch 190 loss: 0.3143609493970871\n",
      "  batch 200 loss: 0.3032712131738663\n",
      "LOSS train 0.3032712131738663 valid 0.3028797209262848\n",
      "ACC train 0.8814912683823529 valid 0.869140625\n",
      "F1 score: 0.8376798587362039\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.301594677567482\n",
      "  batch 20 loss: 0.30513486862182615\n",
      "  batch 30 loss: 0.30070883631706236\n",
      "  batch 40 loss: 0.3056851863861084\n",
      "  batch 50 loss: 0.29541808664798735\n",
      "  batch 60 loss: 0.29554101526737214\n",
      "  batch 70 loss: 0.3087538957595825\n",
      "  batch 80 loss: 0.2926619976758957\n",
      "  batch 90 loss: 0.3066804200410843\n",
      "  batch 100 loss: 0.289573635160923\n",
      "  batch 110 loss: 0.3064189672470093\n",
      "  batch 120 loss: 0.2989575922489166\n",
      "  batch 130 loss: 0.2993322968482971\n",
      "  batch 140 loss: 0.29774793088436124\n",
      "  batch 150 loss: 0.2903560161590576\n",
      "  batch 160 loss: 0.29616689682006836\n",
      "  batch 170 loss: 0.2867464810609818\n",
      "  batch 180 loss: 0.2869396448135376\n",
      "  batch 190 loss: 0.3087933391332626\n",
      "  batch 200 loss: 0.3073093056678772\n",
      "LOSS train 0.3073093056678772 valid 0.2981594204902649\n",
      "ACC train 0.8819125306372549 valid 0.873046875\n",
      "F1 score: 0.845858908921141\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.292986074090004\n",
      "  batch 20 loss: 0.2870639979839325\n",
      "  batch 30 loss: 0.30050242245197295\n",
      "  batch 40 loss: 0.29891400337219237\n",
      "  batch 50 loss: 0.2920153170824051\n",
      "  batch 60 loss: 0.29679363369941714\n",
      "  batch 70 loss: 0.30484906733036043\n",
      "  batch 80 loss: 0.2828884571790695\n",
      "  batch 90 loss: 0.3040466845035553\n",
      "  batch 100 loss: 0.28380719423294065\n",
      "  batch 110 loss: 0.3026215463876724\n",
      "  batch 120 loss: 0.2964971959590912\n",
      "  batch 130 loss: 0.28831244707107545\n",
      "  batch 140 loss: 0.2879000440239906\n",
      "  batch 150 loss: 0.29260651469230653\n",
      "  batch 160 loss: 0.3118297576904297\n",
      "  batch 170 loss: 0.3059113085269928\n",
      "  batch 180 loss: 0.30221368968486784\n",
      "  batch 190 loss: 0.29638659209012985\n",
      "  batch 200 loss: 0.2958151280879974\n",
      "LOSS train 0.2958151280879974 valid 0.2954624593257904\n",
      "ACC train 0.8827646292892157 valid 0.876953125\n",
      "F1 score: 0.8521222867514283\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.2884476348757744\n",
      "  batch 20 loss: 0.2958493992686272\n",
      "  batch 30 loss: 0.2918850392103195\n",
      "  batch 40 loss: 0.27658920735120773\n",
      "  batch 50 loss: 0.2916815638542175\n",
      "  batch 60 loss: 0.2979840189218521\n",
      "  batch 70 loss: 0.2896828919649124\n",
      "  batch 80 loss: 0.28935736417770386\n",
      "  batch 90 loss: 0.28174745142459867\n",
      "  batch 100 loss: 0.3043834507465363\n",
      "  batch 110 loss: 0.2995024859905243\n",
      "  batch 120 loss: 0.29495914578437804\n",
      "  batch 130 loss: 0.28376292288303373\n",
      "  batch 140 loss: 0.2960529327392578\n",
      "  batch 150 loss: 0.29968394339084625\n",
      "  batch 160 loss: 0.28962447196245195\n",
      "  batch 170 loss: 0.2992843002080917\n",
      "  batch 180 loss: 0.2947002053260803\n",
      "  batch 190 loss: 0.29860678017139436\n",
      "  batch 200 loss: 0.3095527976751328\n",
      "LOSS train 0.3095527976751328 valid 0.29312920570373535\n",
      "ACC train 0.8827837775735294 valid 0.87890625\n",
      "F1 score: 0.853728770887209\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.2907010167837143\n",
      "  batch 20 loss: 0.29698882400989535\n",
      "  batch 30 loss: 0.29660959243774415\n",
      "  batch 40 loss: 0.2897800594568253\n",
      "  batch 50 loss: 0.293510377407074\n",
      "  batch 60 loss: 0.2806311100721359\n",
      "  batch 70 loss: 0.2963001847267151\n",
      "  batch 80 loss: 0.28750636577606203\n",
      "  batch 90 loss: 0.2908364087343216\n",
      "  batch 100 loss: 0.29350255727767943\n",
      "  batch 110 loss: 0.2905424624681473\n",
      "  batch 120 loss: 0.28913693130016327\n",
      "  batch 130 loss: 0.2767333000898361\n",
      "  batch 140 loss: 0.2937962651252747\n",
      "  batch 150 loss: 0.2997602552175522\n",
      "  batch 160 loss: 0.2978643447160721\n",
      "  batch 170 loss: 0.2912014782428741\n",
      "  batch 180 loss: 0.29201131165027616\n",
      "  batch 190 loss: 0.29882273375988005\n",
      "  batch 200 loss: 0.3013603240251541\n",
      "LOSS train 0.3013603240251541 valid 0.2912954092025757\n",
      "ACC train 0.8836933210784313 valid 0.87890625\n",
      "F1 score: 0.853728770887209\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.29955521523952483\n",
      "  batch 20 loss: 0.2888778746128082\n",
      "  batch 30 loss: 0.2941037878394127\n",
      "  batch 40 loss: 0.29984559416770934\n",
      "  batch 50 loss: 0.28457245528697966\n",
      "  batch 60 loss: 0.2853283017873764\n",
      "  batch 70 loss: 0.2976329416036606\n",
      "  batch 80 loss: 0.2708042189478874\n",
      "  batch 90 loss: 0.30035958290100095\n",
      "  batch 100 loss: 0.29001041650772097\n",
      "  batch 110 loss: 0.28655092418193817\n",
      "  batch 120 loss: 0.28958353102207185\n",
      "  batch 130 loss: 0.29114680290222167\n",
      "  batch 140 loss: 0.2842304468154907\n",
      "  batch 150 loss: 0.29388000816106796\n",
      "  batch 160 loss: 0.294179430603981\n",
      "  batch 170 loss: 0.2915873184800148\n",
      "  batch 180 loss: 0.2807948648929596\n",
      "  batch 190 loss: 0.294601634144783\n",
      "  batch 200 loss: 0.2844294548034668\n",
      "LOSS train 0.2844294548034668 valid 0.2895036041736603\n",
      "ACC train 0.8843539368872549 valid 0.873046875\n",
      "F1 score: 0.8442259447074472\n",
      "EPOCH 11:\n",
      "  batch 10 loss: 0.2758631736040115\n",
      "  batch 20 loss: 0.2916858077049255\n",
      "  batch 30 loss: 0.29789610505104064\n",
      "  batch 40 loss: 0.30607487857341764\n",
      "  batch 50 loss: 0.29049270749092104\n",
      "  batch 60 loss: 0.2946213483810425\n",
      "  batch 70 loss: 0.2875428706407547\n",
      "  batch 80 loss: 0.30287420749664307\n",
      "  batch 90 loss: 0.2872307479381561\n",
      "  batch 100 loss: 0.27740137577056884\n",
      "  batch 110 loss: 0.2930497363209724\n",
      "  batch 120 loss: 0.30238034427165983\n",
      "  batch 130 loss: 0.27786302119493483\n",
      "  batch 140 loss: 0.2817374110221863\n",
      "  batch 150 loss: 0.2922023504972458\n",
      "  batch 160 loss: 0.2829052433371544\n",
      "  batch 170 loss: 0.2776207774877548\n",
      "  batch 180 loss: 0.28996929824352263\n",
      "  batch 190 loss: 0.2847208186984062\n",
      "  batch 200 loss: 0.2872424781322479\n",
      "LOSS train 0.2872424781322479 valid 0.2873282730579376\n",
      "ACC train 0.8845837162990197 valid 0.8828125\n",
      "F1 score: 0.858447197632783\n",
      "EPOCH 12:\n",
      "  batch 10 loss: 0.29552893191576\n",
      "  batch 20 loss: 0.2777304708957672\n",
      "  batch 30 loss: 0.2985974282026291\n",
      "  batch 40 loss: 0.2752174586057663\n",
      "  batch 50 loss: 0.2889330118894577\n",
      "  batch 60 loss: 0.2919840946793556\n",
      "  batch 70 loss: 0.29942416250705717\n",
      "  batch 80 loss: 0.2871629387140274\n",
      "  batch 90 loss: 0.29986671805381776\n",
      "  batch 100 loss: 0.28635076582431795\n",
      "  batch 110 loss: 0.27903112173080447\n",
      "  batch 120 loss: 0.2866850942373276\n",
      "  batch 130 loss: 0.29550141990184786\n",
      "  batch 140 loss: 0.2779751539230347\n",
      "  batch 150 loss: 0.2896272957324982\n",
      "  batch 160 loss: 0.28609980046749117\n",
      "  batch 170 loss: 0.28843407332897186\n",
      "  batch 180 loss: 0.27595304697752\n",
      "  batch 190 loss: 0.28054588884115217\n",
      "  batch 200 loss: 0.2871626168489456\n",
      "LOSS train 0.2871626168489456 valid 0.2858022153377533\n",
      "ACC train 0.8850528492647058 valid 0.8828125\n",
      "F1 score: 0.858447197632783\n",
      "EPOCH 13:\n",
      "  batch 10 loss: 0.2867499768733978\n",
      "  batch 20 loss: 0.2916123002767563\n",
      "  batch 30 loss: 0.27619987428188325\n",
      "  batch 40 loss: 0.2755234375596046\n",
      "  batch 50 loss: 0.28695122003555296\n",
      "  batch 60 loss: 0.290145543217659\n",
      "  batch 70 loss: 0.2852156668901443\n",
      "  batch 80 loss: 0.2760249063372612\n",
      "  batch 90 loss: 0.277431446313858\n",
      "  batch 100 loss: 0.2841627761721611\n",
      "  batch 110 loss: 0.2919753655791283\n",
      "  batch 120 loss: 0.2796828389167786\n",
      "  batch 130 loss: 0.28744964152574537\n",
      "  batch 140 loss: 0.2827085882425308\n",
      "  batch 150 loss: 0.29328387677669526\n",
      "  batch 160 loss: 0.2915597677230835\n",
      "  batch 170 loss: 0.2872491583228111\n",
      "  batch 180 loss: 0.29943500459194183\n",
      "  batch 190 loss: 0.28262831568717955\n",
      "  batch 200 loss: 0.27083253860473633\n",
      "LOSS train 0.27083253860473633 valid 0.2842910587787628\n",
      "ACC train 0.8859911151960784 valid 0.875\n",
      "F1 score: 0.8440844481605351\n",
      "EPOCH 14:\n",
      "  batch 10 loss: 0.29037677943706514\n",
      "  batch 20 loss: 0.2862540423870087\n",
      "  batch 30 loss: 0.28245595991611483\n",
      "  batch 40 loss: 0.2913897022604942\n",
      "  batch 50 loss: 0.28980847299098966\n",
      "  batch 60 loss: 0.2840953812003136\n",
      "  batch 70 loss: 0.2809841766953468\n",
      "  batch 80 loss: 0.2817149445414543\n",
      "  batch 90 loss: 0.2691174641251564\n",
      "  batch 100 loss: 0.2930590808391571\n",
      "  batch 110 loss: 0.2653668701648712\n",
      "  batch 120 loss: 0.27954675257205963\n",
      "  batch 130 loss: 0.29227126091718675\n",
      "  batch 140 loss: 0.28215882182121277\n",
      "  batch 150 loss: 0.2877797305583954\n",
      "  batch 160 loss: 0.2988795667886734\n",
      "  batch 170 loss: 0.28177642822265625\n",
      "  batch 180 loss: 0.28059165775775907\n",
      "  batch 190 loss: 0.2773278966546059\n",
      "  batch 200 loss: 0.2895075440406799\n",
      "LOSS train 0.2895075440406799 valid 0.2823212146759033\n",
      "ACC train 0.8863549325980392 valid 0.8828125\n",
      "F1 score: 0.858447197632783\n",
      "EPOCH 15:\n",
      "  batch 10 loss: 0.28914614766836166\n",
      "  batch 20 loss: 0.29473095536231997\n",
      "  batch 30 loss: 0.2735292583703995\n",
      "  batch 40 loss: 0.2620418414473534\n",
      "  batch 50 loss: 0.2881822049617767\n",
      "  batch 60 loss: 0.283436530828476\n",
      "  batch 70 loss: 0.28136572390794756\n",
      "  batch 80 loss: 0.2794647112488747\n",
      "  batch 90 loss: 0.2943165138363838\n",
      "  batch 100 loss: 0.27385256588459017\n",
      "  batch 110 loss: 0.29447171539068223\n",
      "  batch 120 loss: 0.2923987865447998\n",
      "  batch 130 loss: 0.2683068037033081\n",
      "  batch 140 loss: 0.2923474103212357\n",
      "  batch 150 loss: 0.2632607832551003\n",
      "  batch 160 loss: 0.29538815170526506\n",
      "  batch 170 loss: 0.2794493228197098\n",
      "  batch 180 loss: 0.29705062210559846\n",
      "  batch 190 loss: 0.2739157766103745\n",
      "  batch 200 loss: 0.2867735892534256\n",
      "LOSS train 0.2867735892534256 valid 0.2809077501296997\n",
      "ACC train 0.8861730238970589 valid 0.880859375\n",
      "F1 score: 0.8553445145259938\n",
      "EPOCH 16:\n",
      "  batch 10 loss: 0.26397702395915984\n",
      "  batch 20 loss: 0.2737667143344879\n",
      "  batch 30 loss: 0.2768888399004936\n",
      "  batch 40 loss: 0.2827898383140564\n",
      "  batch 50 loss: 0.2701961427927017\n",
      "  batch 60 loss: 0.27989705353975297\n",
      "  batch 70 loss: 0.2767244502902031\n",
      "  batch 80 loss: 0.2940182164311409\n",
      "  batch 90 loss: 0.28895512223243713\n",
      "  batch 100 loss: 0.2835913598537445\n",
      "  batch 110 loss: 0.28075633347034457\n",
      "  batch 120 loss: 0.29187542796134947\n",
      "  batch 130 loss: 0.29731938540935515\n",
      "  batch 140 loss: 0.28967219591140747\n",
      "  batch 150 loss: 0.27654611468315127\n",
      "  batch 160 loss: 0.2769298434257507\n",
      "  batch 170 loss: 0.29628426134586333\n",
      "  batch 180 loss: 0.28215167224407195\n",
      "  batch 190 loss: 0.262943235039711\n",
      "  batch 200 loss: 0.27396187037229536\n",
      "LOSS train 0.27396187037229536 valid 0.2791745364665985\n",
      "ACC train 0.8863357843137255 valid 0.88671875\n",
      "F1 score: 0.8658577031521111\n",
      "EPOCH 17:\n",
      "  batch 10 loss: 0.2872565656900406\n",
      "  batch 20 loss: 0.29231939315795896\n",
      "  batch 30 loss: 0.2702122688293457\n",
      "  batch 40 loss: 0.2778369948267937\n",
      "  batch 50 loss: 0.2828508585691452\n",
      "  batch 60 loss: 0.27557902634143827\n",
      "  batch 70 loss: 0.26576596200466157\n",
      "  batch 80 loss: 0.2838173031806946\n",
      "  batch 90 loss: 0.28867781460285186\n",
      "  batch 100 loss: 0.2768195778131485\n",
      "  batch 110 loss: 0.2798100858926773\n",
      "  batch 120 loss: 0.2754373997449875\n",
      "  batch 130 loss: 0.28574844151735307\n",
      "  batch 140 loss: 0.2760047197341919\n",
      "  batch 150 loss: 0.2802023097872734\n",
      "  batch 160 loss: 0.27773965895175934\n",
      "  batch 170 loss: 0.2833714231848717\n",
      "  batch 180 loss: 0.2825587302446365\n",
      "  batch 190 loss: 0.2907171830534935\n",
      "  batch 200 loss: 0.2786578208208084\n",
      "LOSS train 0.2786578208208084 valid 0.27789387106895447\n",
      "ACC train 0.8865368412990197 valid 0.884765625\n",
      "F1 score: 0.8641968374826612\n",
      "EPOCH 18:\n",
      "  batch 10 loss: 0.26831364929676055\n",
      "  batch 20 loss: 0.2841572776436806\n",
      "  batch 30 loss: 0.2883797913789749\n",
      "  batch 40 loss: 0.2715362191200256\n",
      "  batch 50 loss: 0.2864985913038254\n",
      "  batch 60 loss: 0.2707980215549469\n",
      "  batch 70 loss: 0.2890153169631958\n",
      "  batch 80 loss: 0.2801817998290062\n",
      "  batch 90 loss: 0.27218439877033235\n",
      "  batch 100 loss: 0.27890569120645525\n",
      "  batch 110 loss: 0.2778718262910843\n",
      "  batch 120 loss: 0.2841633901000023\n",
      "  batch 130 loss: 0.27890488803386687\n",
      "  batch 140 loss: 0.27451004683971403\n",
      "  batch 150 loss: 0.2809986054897308\n",
      "  batch 160 loss: 0.27766160666942596\n",
      "  batch 170 loss: 0.27882939428091047\n",
      "  batch 180 loss: 0.2842539846897125\n",
      "  batch 190 loss: 0.27619082033634185\n",
      "  batch 200 loss: 0.2769296482205391\n",
      "LOSS train 0.2769296482205391 valid 0.27629780769348145\n",
      "ACC train 0.8871017156862745 valid 0.8828125\n",
      "F1 score: 0.8598669525375939\n",
      "EPOCH 19:\n",
      "  batch 10 loss: 0.2785202294588089\n",
      "  batch 20 loss: 0.29024831503629683\n",
      "  batch 30 loss: 0.29195961356163025\n",
      "  batch 40 loss: 0.2703264161944389\n",
      "  batch 50 loss: 0.2617335841059685\n",
      "  batch 60 loss: 0.2864930212497711\n",
      "  batch 70 loss: 0.27400792241096494\n",
      "  batch 80 loss: 0.27721757888793946\n",
      "  batch 90 loss: 0.27974431216716766\n",
      "  batch 100 loss: 0.28612095713615415\n",
      "  batch 110 loss: 0.27062532901763914\n",
      "  batch 120 loss: 0.2794743269681931\n",
      "  batch 130 loss: 0.27990224957466125\n",
      "  batch 140 loss: 0.283369705080986\n",
      "  batch 150 loss: 0.28558616042137147\n",
      "  batch 160 loss: 0.2696615159511566\n",
      "  batch 170 loss: 0.2784124881029129\n",
      "  batch 180 loss: 0.27428034245967864\n",
      "  batch 190 loss: 0.26499477624893186\n",
      "  batch 200 loss: 0.2880883052945137\n",
      "LOSS train 0.2880883052945137 valid 0.27489566802978516\n",
      "ACC train 0.8876953125 valid 0.8828125\n",
      "F1 score: 0.858447197632783\n",
      "EPOCH 20:\n",
      "  batch 10 loss: 0.27977870404720306\n",
      "  batch 20 loss: 0.26742775589227674\n",
      "  batch 30 loss: 0.2775147080421448\n",
      "  batch 40 loss: 0.2857353210449219\n",
      "  batch 50 loss: 0.2729981794953346\n",
      "  batch 60 loss: 0.2616935700178146\n",
      "  batch 70 loss: 0.2581633538007736\n",
      "  batch 80 loss: 0.28726893961429595\n",
      "  batch 90 loss: 0.2812873676419258\n",
      "  batch 100 loss: 0.27673115134239196\n",
      "  batch 110 loss: 0.274975149333477\n",
      "  batch 120 loss: 0.26872260570526124\n",
      "  batch 130 loss: 0.2866756126284599\n",
      "  batch 140 loss: 0.2816688358783722\n",
      "  batch 150 loss: 0.2803691804409027\n",
      "  batch 160 loss: 0.27535266876220704\n",
      "  batch 170 loss: 0.28158511966466904\n",
      "  batch 180 loss: 0.28784348368644713\n",
      "  batch 190 loss: 0.2700716659426689\n",
      "  batch 200 loss: 0.2671266108751297\n",
      "LOSS train 0.2671266108751297 valid 0.27358776330947876\n",
      "ACC train 0.8880495557598039 valid 0.8828125\n",
      "F1 score: 0.858447197632783\n",
      "EPOCH 21:\n",
      "  batch 10 loss: 0.27395913898944857\n",
      "  batch 20 loss: 0.2649774238467216\n",
      "  batch 30 loss: 0.27314009964466096\n",
      "  batch 40 loss: 0.27203691750764847\n",
      "  batch 50 loss: 0.27992640882730485\n",
      "  batch 60 loss: 0.27336925715208055\n",
      "  batch 70 loss: 0.27779689729213713\n",
      "  batch 80 loss: 0.2780921682715416\n",
      "  batch 90 loss: 0.2741715118288994\n",
      "  batch 100 loss: 0.27677892297506335\n",
      "  batch 110 loss: 0.28080596029758453\n",
      "  batch 120 loss: 0.2893144264817238\n",
      "  batch 130 loss: 0.26997471749782564\n",
      "  batch 140 loss: 0.2730526626110077\n",
      "  batch 150 loss: 0.2732613533735275\n",
      "  batch 160 loss: 0.2973028361797333\n",
      "  batch 170 loss: 0.2713767722249031\n",
      "  batch 180 loss: 0.26886571049690244\n",
      "  batch 190 loss: 0.27837587893009186\n",
      "  batch 200 loss: 0.2684630036354065\n",
      "LOSS train 0.2684630036354065 valid 0.2724135220050812\n",
      "ACC train 0.8888633578431373 valid 0.884765625\n",
      "F1 score: 0.8615113479100678\n",
      "EPOCH 22:\n",
      "  batch 10 loss: 0.2712604746222496\n",
      "  batch 20 loss: 0.28423896729946135\n",
      "  batch 30 loss: 0.2737486377358437\n",
      "  batch 40 loss: 0.27688696384429934\n",
      "  batch 50 loss: 0.26286416202783586\n",
      "  batch 60 loss: 0.2748316615819931\n",
      "  batch 70 loss: 0.2739483416080475\n",
      "  batch 80 loss: 0.2733723923563957\n",
      "  batch 90 loss: 0.2724244698882103\n",
      "  batch 100 loss: 0.27248531877994536\n",
      "  batch 110 loss: 0.2789566397666931\n",
      "  batch 120 loss: 0.2849603295326233\n",
      "  batch 130 loss: 0.2843194767832756\n",
      "  batch 140 loss: 0.26236148923635483\n",
      "  batch 150 loss: 0.28249981701374055\n",
      "  batch 160 loss: 0.27950828373432157\n",
      "  batch 170 loss: 0.2654423892498016\n",
      "  batch 180 loss: 0.2605335399508476\n",
      "  batch 190 loss: 0.272371181845665\n",
      "  batch 200 loss: 0.29180554747581483\n",
      "LOSS train 0.29180554747581483 valid 0.2709205150604248\n",
      "ACC train 0.8888154871323529 valid 0.8828125\n",
      "F1 score: 0.8598669525375939\n",
      "EPOCH 23:\n",
      "  batch 10 loss: 0.260401351749897\n",
      "  batch 20 loss: 0.27020283639431\n",
      "  batch 30 loss: 0.2717879891395569\n",
      "  batch 40 loss: 0.2772719621658325\n",
      "  batch 50 loss: 0.2721980556845665\n",
      "  batch 60 loss: 0.27712910175323485\n",
      "  batch 70 loss: 0.27142110764980315\n",
      "  batch 80 loss: 0.27282755821943283\n",
      "  batch 90 loss: 0.28751168251037595\n",
      "  batch 100 loss: 0.26755939722061156\n",
      "  batch 110 loss: 0.2692842036485672\n",
      "  batch 120 loss: 0.2731427997350693\n",
      "  batch 130 loss: 0.2821800738573074\n",
      "  batch 140 loss: 0.2707786113023758\n",
      "  batch 150 loss: 0.266747359931469\n",
      "  batch 160 loss: 0.27055185884237287\n",
      "  batch 170 loss: 0.2831869304180145\n",
      "  batch 180 loss: 0.2818632826209068\n",
      "  batch 190 loss: 0.26664898097515105\n",
      "  batch 200 loss: 0.26488217115402224\n",
      "LOSS train 0.26488217115402224 valid 0.27004101872444153\n",
      "ACC train 0.8896388633578431 valid 0.890625\n",
      "F1 score: 0.8692091557017543\n",
      "EPOCH 24:\n",
      "  batch 10 loss: 0.25673583894968033\n",
      "  batch 20 loss: 0.2567001849412918\n",
      "  batch 30 loss: 0.2829394966363907\n",
      "  batch 40 loss: 0.2691354215145111\n",
      "  batch 50 loss: 0.2546845480799675\n",
      "  batch 60 loss: 0.27042670994997026\n",
      "  batch 70 loss: 0.26908617168664933\n",
      "  batch 80 loss: 0.2731330573558807\n",
      "  batch 90 loss: 0.28213019818067553\n",
      "  batch 100 loss: 0.27796673476696016\n",
      "  batch 110 loss: 0.2709600359201431\n",
      "  batch 120 loss: 0.2573744565248489\n",
      "  batch 130 loss: 0.28704385459423065\n",
      "  batch 140 loss: 0.27203158289194107\n",
      "  batch 150 loss: 0.2621994152665138\n",
      "  batch 160 loss: 0.29499229937791827\n",
      "  batch 170 loss: 0.2757931649684906\n",
      "  batch 180 loss: 0.2866064414381981\n",
      "  batch 190 loss: 0.2743846714496613\n",
      "  batch 200 loss: 0.2763585805892944\n",
      "LOSS train 0.2763585805892944 valid 0.2685883641242981\n",
      "ACC train 0.8894186580882353 valid 0.890625\n",
      "F1 score: 0.8717090897007442\n",
      "EPOCH 25:\n",
      "  batch 10 loss: 0.26364503502845765\n",
      "  batch 20 loss: 0.27034404277801516\n",
      "  batch 30 loss: 0.2658829569816589\n",
      "  batch 40 loss: 0.27046264708042145\n",
      "  batch 50 loss: 0.2790213733911514\n",
      "  batch 60 loss: 0.27426562905311586\n",
      "  batch 70 loss: 0.2636151537299156\n",
      "  batch 80 loss: 0.268928661942482\n",
      "  batch 90 loss: 0.2726029932498932\n",
      "  batch 100 loss: 0.28135933429002763\n",
      "  batch 110 loss: 0.2622621297836304\n",
      "  batch 120 loss: 0.2744744628667831\n",
      "  batch 130 loss: 0.269697867333889\n",
      "  batch 140 loss: 0.2735598564147949\n",
      "  batch 150 loss: 0.2655310407280922\n",
      "  batch 160 loss: 0.26185307651758194\n",
      "  batch 170 loss: 0.264107471704483\n",
      "  batch 180 loss: 0.2802055194973946\n",
      "  batch 190 loss: 0.2757214456796646\n",
      "  batch 200 loss: 0.2799530416727066\n",
      "LOSS train 0.2799530416727066 valid 0.2669733464717865\n",
      "ACC train 0.8904047947303921 valid 0.890625\n",
      "F1 score: 0.8692091557017543\n",
      "EPOCH 26:\n",
      "  batch 10 loss: 0.27805828899145124\n",
      "  batch 20 loss: 0.27328979074954984\n",
      "  batch 30 loss: 0.27004761546850203\n",
      "  batch 40 loss: 0.27072787284851074\n",
      "  batch 50 loss: 0.2730184316635132\n",
      "  batch 60 loss: 0.2811629757285118\n",
      "  batch 70 loss: 0.25485561937093737\n",
      "  batch 80 loss: 0.274209326505661\n",
      "  batch 90 loss: 0.27210413217544555\n",
      "  batch 100 loss: 0.2596223682165146\n",
      "  batch 110 loss: 0.2672671228647232\n",
      "  batch 120 loss: 0.27400221824646\n",
      "  batch 130 loss: 0.27113551944494246\n",
      "  batch 140 loss: 0.26612027138471606\n",
      "  batch 150 loss: 0.26297903507947923\n",
      "  batch 160 loss: 0.2631273567676544\n",
      "  batch 170 loss: 0.26206122785806657\n",
      "  batch 180 loss: 0.2686138033866882\n",
      "  batch 190 loss: 0.26145123690366745\n",
      "  batch 200 loss: 0.281541121006012\n",
      "LOSS train 0.281541121006012 valid 0.26552367210388184\n",
      "ACC train 0.8911515778186274 valid 0.892578125\n",
      "F1 score: 0.8721764942128168\n",
      "EPOCH 27:\n",
      "  batch 10 loss: 0.27611754536628724\n",
      "  batch 20 loss: 0.2619589105248451\n",
      "  batch 30 loss: 0.2777296006679535\n",
      "  batch 40 loss: 0.26687792837619784\n",
      "  batch 50 loss: 0.27023197561502454\n",
      "  batch 60 loss: 0.2528271660208702\n",
      "  batch 70 loss: 0.29184119701385497\n",
      "  batch 80 loss: 0.28047193586826324\n",
      "  batch 90 loss: 0.2653829574584961\n",
      "  batch 100 loss: 0.26906210780143736\n",
      "  batch 110 loss: 0.2690610647201538\n",
      "  batch 120 loss: 0.27739628255367277\n",
      "  batch 130 loss: 0.27571905553340914\n",
      "  batch 140 loss: 0.2752565175294876\n",
      "  batch 150 loss: 0.2523516908288002\n",
      "  batch 160 loss: 0.2728109911084175\n",
      "  batch 170 loss: 0.2755652338266373\n",
      "  batch 180 loss: 0.2608800813555717\n",
      "  batch 190 loss: 0.27010240256786344\n",
      "  batch 200 loss: 0.2498062178492546\n",
      "LOSS train 0.2498062178492546 valid 0.2645873427391052\n",
      "ACC train 0.890347349877451 valid 0.888671875\n",
      "F1 score: 0.8675283667296465\n",
      "EPOCH 28:\n",
      "  batch 10 loss: 0.26850770264863966\n",
      "  batch 20 loss: 0.27575997114181516\n",
      "  batch 30 loss: 0.2669247552752495\n",
      "  batch 40 loss: 0.2673137798905373\n",
      "  batch 50 loss: 0.26274501979351045\n",
      "  batch 60 loss: 0.27432091534137726\n",
      "  batch 70 loss: 0.26102636009454727\n",
      "  batch 80 loss: 0.25235377699136735\n",
      "  batch 90 loss: 0.2559486851096153\n",
      "  batch 100 loss: 0.2701564371585846\n",
      "  batch 110 loss: 0.28602908849716185\n",
      "  batch 120 loss: 0.2603835478425026\n",
      "  batch 130 loss: 0.27433316707611083\n",
      "  batch 140 loss: 0.2535898178815842\n",
      "  batch 150 loss: 0.27517148554325105\n",
      "  batch 160 loss: 0.2597048103809357\n",
      "  batch 170 loss: 0.2862760692834854\n",
      "  batch 180 loss: 0.26640319228172304\n",
      "  batch 190 loss: 0.26204576194286344\n",
      "  batch 200 loss: 0.26057071387767794\n",
      "LOSS train 0.26057071387767794 valid 0.2638000249862671\n",
      "ACC train 0.8914196537990197 valid 0.89453125\n",
      "F1 score: 0.8738802572838347\n",
      "EPOCH 29:\n",
      "  batch 10 loss: 0.25975813567638395\n",
      "  batch 20 loss: 0.2698049068450928\n",
      "  batch 30 loss: 0.26564023792743685\n",
      "  batch 40 loss: 0.2488194689154625\n",
      "  batch 50 loss: 0.27944287955760955\n",
      "  batch 60 loss: 0.27134409695863726\n",
      "  batch 70 loss: 0.2641011893749237\n",
      "  batch 80 loss: 0.26998846530914306\n",
      "  batch 90 loss: 0.2750305339694023\n",
      "  batch 100 loss: 0.2648981049656868\n",
      "  batch 110 loss: 0.2666257068514824\n",
      "  batch 120 loss: 0.26455081552267073\n",
      "  batch 130 loss: 0.25787835121154784\n",
      "  batch 140 loss: 0.26769348233938217\n",
      "  batch 150 loss: 0.26435932964086534\n",
      "  batch 160 loss: 0.2723034366965294\n",
      "  batch 170 loss: 0.26694640666246416\n",
      "  batch 180 loss: 0.2634754925966263\n",
      "  batch 190 loss: 0.26665019392967226\n",
      "  batch 200 loss: 0.2637678250670433\n",
      "LOSS train 0.2637678250670433 valid 0.26176539063453674\n",
      "ACC train 0.8923483455882353 valid 0.888671875\n",
      "F1 score: 0.8688003345171474\n",
      "EPOCH 30:\n",
      "  batch 10 loss: 0.2561458691954613\n",
      "  batch 20 loss: 0.2582800179719925\n",
      "  batch 30 loss: 0.28133689761161806\n",
      "  batch 40 loss: 0.26849946230649946\n",
      "  batch 50 loss: 0.25652056336402895\n",
      "  batch 60 loss: 0.2564905658364296\n",
      "  batch 70 loss: 0.26437161713838575\n",
      "  batch 80 loss: 0.2569606423377991\n",
      "  batch 90 loss: 0.2671994000673294\n",
      "  batch 100 loss: 0.2735490471124649\n",
      "  batch 110 loss: 0.2565708115696907\n",
      "  batch 120 loss: 0.2623631924390793\n",
      "  batch 130 loss: 0.2672398641705513\n",
      "  batch 140 loss: 0.2599863350391388\n",
      "  batch 150 loss: 0.26547177582979203\n",
      "  batch 160 loss: 0.27634465843439104\n",
      "  batch 170 loss: 0.2700268626213074\n",
      "  batch 180 loss: 0.27439403533935547\n",
      "  batch 190 loss: 0.2652278199791908\n",
      "  batch 200 loss: 0.2696305617690086\n",
      "LOSS train 0.2696305617690086 valid 0.2607569396495819\n",
      "ACC train 0.8926259957107843 valid 0.892578125\n",
      "F1 score: 0.8734038315516334\n",
      "EPOCH 31:\n",
      "  batch 10 loss: 0.2665350824594498\n",
      "  batch 20 loss: 0.2758270114660263\n",
      "  batch 30 loss: 0.26280532479286195\n",
      "  batch 40 loss: 0.27211150228977204\n",
      "  batch 50 loss: 0.2752338796854019\n",
      "  batch 60 loss: 0.2454762265086174\n",
      "  batch 70 loss: 0.2602137178182602\n",
      "  batch 80 loss: 0.26248457580804824\n",
      "  batch 90 loss: 0.2500602319836617\n",
      "  batch 100 loss: 0.25967680662870407\n",
      "  batch 110 loss: 0.2672708988189697\n",
      "  batch 120 loss: 0.2613424092531204\n",
      "  batch 130 loss: 0.2556564584374428\n",
      "  batch 140 loss: 0.2735262557864189\n",
      "  batch 150 loss: 0.2619428843259811\n",
      "  batch 160 loss: 0.2905778527259827\n",
      "  batch 170 loss: 0.2587495520710945\n",
      "  batch 180 loss: 0.277297243475914\n",
      "  batch 190 loss: 0.25769179612398146\n",
      "  batch 200 loss: 0.2590275153517723\n",
      "LOSS train 0.2590275153517723 valid 0.2597147822380066\n",
      "ACC train 0.8925015318627451 valid 0.892578125\n",
      "F1 score: 0.8734038315516334\n",
      "EPOCH 32:\n",
      "  batch 10 loss: 0.27522545903921125\n",
      "  batch 20 loss: 0.24857133775949478\n",
      "  batch 30 loss: 0.257129992544651\n",
      "  batch 40 loss: 0.2622018873691559\n",
      "  batch 50 loss: 0.2751358598470688\n",
      "  batch 60 loss: 0.26408240795135496\n",
      "  batch 70 loss: 0.24888913184404374\n",
      "  batch 80 loss: 0.27507627904415133\n",
      "  batch 90 loss: 0.2758401781320572\n",
      "  batch 100 loss: 0.2620555102825165\n",
      "  batch 110 loss: 0.27344491630792617\n",
      "  batch 120 loss: 0.2548371747136116\n",
      "  batch 130 loss: 0.25497915893793105\n",
      "  batch 140 loss: 0.2520641773939133\n",
      "  batch 150 loss: 0.24633422642946243\n",
      "  batch 160 loss: 0.2608853578567505\n",
      "  batch 170 loss: 0.2666174918413162\n",
      "  batch 180 loss: 0.26221143901348115\n",
      "  batch 190 loss: 0.2666247859597206\n",
      "  batch 200 loss: 0.2830709144473076\n",
      "LOSS train 0.2830709144473076 valid 0.25858941674232483\n",
      "ACC train 0.8928940716911765 valid 0.890625\n",
      "F1 score: 0.8704832995951417\n",
      "EPOCH 33:\n",
      "  batch 10 loss: 0.2680532544851303\n",
      "  batch 20 loss: 0.2543253690004349\n",
      "  batch 30 loss: 0.27418061047792436\n",
      "  batch 40 loss: 0.2554867848753929\n",
      "  batch 50 loss: 0.2652900442481041\n",
      "  batch 60 loss: 0.2635524898767471\n",
      "  batch 70 loss: 0.27960105389356615\n",
      "  batch 80 loss: 0.2668421551585197\n",
      "  batch 90 loss: 0.2672955334186554\n",
      "  batch 100 loss: 0.25628791898489\n",
      "  batch 110 loss: 0.2691252395510674\n",
      "  batch 120 loss: 0.2624860301613808\n",
      "  batch 130 loss: 0.2477120652794838\n",
      "  batch 140 loss: 0.2599945217370987\n",
      "  batch 150 loss: 0.2546336740255356\n",
      "  batch 160 loss: 0.2657655641436577\n",
      "  batch 170 loss: 0.26150853782892225\n",
      "  batch 180 loss: 0.26507779210805893\n",
      "  batch 190 loss: 0.2457423910498619\n",
      "  batch 200 loss: 0.25661981999874117\n",
      "LOSS train 0.25661981999874117 valid 0.25845077633857727\n",
      "ACC train 0.8931908700980392 valid 0.8984375\n",
      "F1 score: 0.8808727261506911\n",
      "EPOCH 34:\n",
      "  batch 10 loss: 0.2667522355914116\n",
      "  batch 20 loss: 0.26357765644788744\n",
      "  batch 30 loss: 0.2503078103065491\n",
      "  batch 40 loss: 0.25897167772054674\n",
      "  batch 50 loss: 0.2648491322994232\n",
      "  batch 60 loss: 0.2594697505235672\n",
      "  batch 70 loss: 0.27889983505010607\n",
      "  batch 80 loss: 0.26107748299837114\n",
      "  batch 90 loss: 0.26812816262245176\n",
      "  batch 100 loss: 0.2642791509628296\n",
      "  batch 110 loss: 0.26675747483968737\n",
      "  batch 120 loss: 0.27055561542510986\n",
      "  batch 130 loss: 0.24914595633745193\n",
      "  batch 140 loss: 0.26186350733041763\n",
      "  batch 150 loss: 0.2543371692299843\n",
      "  batch 160 loss: 0.2613003820180893\n",
      "  batch 170 loss: 0.2588144615292549\n",
      "  batch 180 loss: 0.2487005725502968\n",
      "  batch 190 loss: 0.27044337540864943\n",
      "  batch 200 loss: 0.2582805812358856\n",
      "LOSS train 0.2582805812358856 valid 0.2572284936904907\n",
      "ACC train 0.8934876685049019 valid 0.896484375\n",
      "F1 score: 0.8780073285861195\n",
      "EPOCH 35:\n",
      "  batch 10 loss: 0.26030769646167756\n",
      "  batch 20 loss: 0.24448009133338927\n",
      "  batch 30 loss: 0.252209410071373\n",
      "  batch 40 loss: 0.2748738631606102\n",
      "  batch 50 loss: 0.2635936915874481\n",
      "  batch 60 loss: 0.2673739716410637\n",
      "  batch 70 loss: 0.25003580302000045\n",
      "  batch 80 loss: 0.2570128202438354\n",
      "  batch 90 loss: 0.25041802376508715\n",
      "  batch 100 loss: 0.2587182059884071\n",
      "  batch 110 loss: 0.25352672934532167\n",
      "  batch 120 loss: 0.2736350253224373\n",
      "  batch 130 loss: 0.2607476308941841\n",
      "  batch 140 loss: 0.257219135761261\n",
      "  batch 150 loss: 0.27589686065912244\n",
      "  batch 160 loss: 0.26681779474020006\n",
      "  batch 170 loss: 0.270135298371315\n",
      "  batch 180 loss: 0.2607608586549759\n",
      "  batch 190 loss: 0.2568174794316292\n",
      "  batch 200 loss: 0.2592677906155586\n",
      "LOSS train 0.2592677906155586 valid 0.2563372552394867\n",
      "ACC train 0.8941865808823529 valid 0.89453125\n",
      "F1 score: 0.8751088960381724\n",
      "EPOCH 36:\n",
      "  batch 10 loss: 0.25211143046617507\n",
      "  batch 20 loss: 0.2514245092868805\n",
      "  batch 30 loss: 0.26233411729335787\n",
      "  batch 40 loss: 0.253569121658802\n",
      "  batch 50 loss: 0.2619572624564171\n",
      "  batch 60 loss: 0.25511831492185594\n",
      "  batch 70 loss: 0.2590277150273323\n",
      "  batch 80 loss: 0.25238377302885057\n",
      "  batch 90 loss: 0.2632428973913193\n",
      "  batch 100 loss: 0.2609857976436615\n",
      "  batch 110 loss: 0.2603106558322906\n",
      "  batch 120 loss: 0.25485439151525496\n",
      "  batch 130 loss: 0.26341702938079836\n",
      "  batch 140 loss: 0.27323468327522277\n",
      "  batch 150 loss: 0.25349318236112595\n",
      "  batch 160 loss: 0.2636033073067665\n",
      "  batch 170 loss: 0.2751991704106331\n",
      "  batch 180 loss: 0.25082328021526334\n",
      "  batch 190 loss: 0.26291377246379855\n",
      "  batch 200 loss: 0.2551133051514626\n",
      "LOSS train 0.2551133051514626 valid 0.25457656383514404\n",
      "ACC train 0.8948759191176471 valid 0.8984375\n",
      "F1 score: 0.8808727261506911\n",
      "EPOCH 37:\n",
      "  batch 10 loss: 0.25964105129241943\n",
      "  batch 20 loss: 0.2553060784935951\n",
      "  batch 30 loss: 0.26238237917423246\n",
      "  batch 40 loss: 0.2635125800967216\n",
      "  batch 50 loss: 0.2750172480940819\n",
      "  batch 60 loss: 0.2573227509856224\n",
      "  batch 70 loss: 0.25289065986871717\n",
      "  batch 80 loss: 0.2629517957568169\n",
      "  batch 90 loss: 0.2559869572520256\n",
      "  batch 100 loss: 0.25526794493198396\n",
      "  batch 110 loss: 0.2617391496896744\n",
      "  batch 120 loss: 0.264192721247673\n",
      "  batch 130 loss: 0.264530049264431\n",
      "  batch 140 loss: 0.23955940157175065\n",
      "  batch 150 loss: 0.2566290348768234\n",
      "  batch 160 loss: 0.25907890498638153\n",
      "  batch 170 loss: 0.2614977926015854\n",
      "  batch 180 loss: 0.25490173846483233\n",
      "  batch 190 loss: 0.2653904601931572\n",
      "  batch 200 loss: 0.25428124219179155\n",
      "LOSS train 0.25428124219179155 valid 0.2535209059715271\n",
      "ACC train 0.8949046415441176 valid 0.890625\n",
      "F1 score: 0.8692091557017543\n",
      "EPOCH 38:\n",
      "  batch 10 loss: 0.2753933548927307\n",
      "  batch 20 loss: 0.26309664994478227\n",
      "  batch 30 loss: 0.25570630133152006\n",
      "  batch 40 loss: 0.25985800474882126\n",
      "  batch 50 loss: 0.2588335633277893\n",
      "  batch 60 loss: 0.2528658345341682\n",
      "  batch 70 loss: 0.2545058712363243\n",
      "  batch 80 loss: 0.27018226236104964\n",
      "  batch 90 loss: 0.25976922512054446\n",
      "  batch 100 loss: 0.2556116834282875\n",
      "  batch 110 loss: 0.2569279745221138\n",
      "  batch 120 loss: 0.2567170709371567\n",
      "  batch 130 loss: 0.2490654170513153\n",
      "  batch 140 loss: 0.2481376886367798\n",
      "  batch 150 loss: 0.25001530051231385\n",
      "  batch 160 loss: 0.26817668080329893\n",
      "  batch 170 loss: 0.2500545740127563\n",
      "  batch 180 loss: 0.2634324491024017\n",
      "  batch 190 loss: 0.251554436981678\n",
      "  batch 200 loss: 0.2587852865457535\n",
      "LOSS train 0.2587852865457535 valid 0.25292542576789856\n",
      "ACC train 0.8952588848039216 valid 0.900390625\n",
      "F1 score: 0.8837059913281315\n",
      "EPOCH 39:\n",
      "  batch 10 loss: 0.25497990995645525\n",
      "  batch 20 loss: 0.26019004732370377\n",
      "  batch 30 loss: 0.24161986708641053\n",
      "  batch 40 loss: 0.25143310576677325\n",
      "  batch 50 loss: 0.2495725706219673\n",
      "  batch 60 loss: 0.2363389566540718\n",
      "  batch 70 loss: 0.2570273205637932\n",
      "  batch 80 loss: 0.2603891909122467\n",
      "  batch 90 loss: 0.24976123720407487\n",
      "  batch 100 loss: 0.2611362159252167\n",
      "  batch 110 loss: 0.25493840128183365\n",
      "  batch 120 loss: 0.2581624835729599\n",
      "  batch 130 loss: 0.25701492577791213\n",
      "  batch 140 loss: 0.25417217761278155\n",
      "  batch 150 loss: 0.2533652365207672\n",
      "  batch 160 loss: 0.26923288553953173\n",
      "  batch 170 loss: 0.2614664301276207\n",
      "  batch 180 loss: 0.2600408852100372\n",
      "  batch 190 loss: 0.2685187295079231\n",
      "  batch 200 loss: 0.2635312959551811\n",
      "LOSS train 0.2635312959551811 valid 0.2515726685523987\n",
      "ACC train 0.8954886642156863 valid 0.89453125\n",
      "F1 score: 0.8751088960381724\n",
      "EPOCH 40:\n",
      "  batch 10 loss: 0.2595310971140862\n",
      "  batch 20 loss: 0.25387795120477674\n",
      "  batch 30 loss: 0.26416576355695726\n",
      "  batch 40 loss: 0.2576384380459785\n",
      "  batch 50 loss: 0.25437234789133073\n",
      "  batch 60 loss: 0.2480594575405121\n",
      "  batch 70 loss: 0.2549589455127716\n",
      "  batch 80 loss: 0.24631026685237883\n",
      "  batch 90 loss: 0.25408382415771485\n",
      "  batch 100 loss: 0.2573005735874176\n",
      "  batch 110 loss: 0.24606026709079742\n",
      "  batch 120 loss: 0.25686979442834856\n",
      "  batch 130 loss: 0.25540721565485003\n",
      "  batch 140 loss: 0.24472486674785615\n",
      "  batch 150 loss: 0.2653178423643112\n",
      "  batch 160 loss: 0.2621155172586441\n",
      "  batch 170 loss: 0.2598430633544922\n",
      "  batch 180 loss: 0.25876588821411134\n",
      "  batch 190 loss: 0.26273540407419205\n",
      "  batch 200 loss: 0.25214916467666626\n",
      "LOSS train 0.25214916467666626 valid 0.25057733058929443\n",
      "ACC train 0.8955748314950981 valid 0.896484375\n",
      "F1 score: 0.8780073285861195\n",
      "EPOCH 41:\n",
      "  batch 10 loss: 0.26993702352046967\n",
      "  batch 20 loss: 0.2489300400018692\n",
      "  batch 30 loss: 0.2579174906015396\n",
      "  batch 40 loss: 0.2576538220047951\n",
      "  batch 50 loss: 0.2516693502664566\n",
      "  batch 60 loss: 0.253170146048069\n",
      "  batch 70 loss: 0.25011212974786756\n",
      "  batch 80 loss: 0.2583567649126053\n",
      "  batch 90 loss: 0.24493435323238372\n",
      "  batch 100 loss: 0.24595666080713272\n",
      "  batch 110 loss: 0.2545609325170517\n",
      "  batch 120 loss: 0.25579933822155\n",
      "  batch 130 loss: 0.2638305127620697\n",
      "  batch 140 loss: 0.2592824727296829\n",
      "  batch 150 loss: 0.25461298376321795\n",
      "  batch 160 loss: 0.2541273504495621\n",
      "  batch 170 loss: 0.25953095555305483\n",
      "  batch 180 loss: 0.2579397991299629\n",
      "  batch 190 loss: 0.25152847170829773\n",
      "  batch 200 loss: 0.254407511651516\n",
      "LOSS train 0.254407511651516 valid 0.25109195709228516\n",
      "ACC train 0.896695006127451 valid 0.90234375\n",
      "F1 score: 0.8865079963613575\n",
      "EPOCH 42:\n",
      "  batch 10 loss: 0.27186465710401536\n",
      "  batch 20 loss: 0.25300870686769483\n",
      "  batch 30 loss: 0.26042641550302503\n",
      "  batch 40 loss: 0.2527739107608795\n",
      "  batch 50 loss: 0.2630128338932991\n",
      "  batch 60 loss: 0.24428424686193467\n",
      "  batch 70 loss: 0.2546672701835632\n",
      "  batch 80 loss: 0.2473236307501793\n",
      "  batch 90 loss: 0.24911377727985382\n",
      "  batch 100 loss: 0.25482558757066726\n",
      "  batch 110 loss: 0.2627438962459564\n",
      "  batch 120 loss: 0.24677511304616928\n",
      "  batch 130 loss: 0.2548887446522713\n",
      "  batch 140 loss: 0.26200216710567475\n",
      "  batch 150 loss: 0.25713654011487963\n",
      "  batch 160 loss: 0.2619959250092506\n",
      "  batch 170 loss: 0.2509464293718338\n",
      "  batch 180 loss: 0.24874357730150223\n",
      "  batch 190 loss: 0.2519341245293617\n",
      "  batch 200 loss: 0.253248792886734\n",
      "LOSS train 0.253248792886734 valid 0.24882817268371582\n",
      "ACC train 0.8965801164215687 valid 0.90234375\n",
      "F1 score: 0.8865079963613575\n",
      "EPOCH 43:\n",
      "  batch 10 loss: 0.2480111002922058\n",
      "  batch 20 loss: 0.25510468035936357\n",
      "  batch 30 loss: 0.24073124974966048\n",
      "  batch 40 loss: 0.2511745572090149\n",
      "  batch 50 loss: 0.24790524691343307\n",
      "  batch 60 loss: 0.26864455044269564\n",
      "  batch 70 loss: 0.2415065661072731\n",
      "  batch 80 loss: 0.24915379732847215\n",
      "  batch 90 loss: 0.2292676165699959\n",
      "  batch 100 loss: 0.25127060860395434\n",
      "  batch 110 loss: 0.25549264848232267\n",
      "  batch 120 loss: 0.27219809740781786\n",
      "  batch 130 loss: 0.2552380755543709\n",
      "  batch 140 loss: 0.25413312911987307\n",
      "  batch 150 loss: 0.25629148483276365\n",
      "  batch 160 loss: 0.25825362652540207\n",
      "  batch 170 loss: 0.26428722888231276\n",
      "  batch 180 loss: 0.25665935724973676\n",
      "  batch 190 loss: 0.243601493537426\n",
      "  batch 200 loss: 0.2532340049743652\n",
      "LOSS train 0.2532340049743652 valid 0.24785979092121124\n",
      "ACC train 0.8967237285539216 valid 0.90234375\n",
      "F1 score: 0.8865079963613575\n",
      "EPOCH 44:\n",
      "  batch 10 loss: 0.25699810683727264\n",
      "  batch 20 loss: 0.2511286556720734\n",
      "  batch 30 loss: 0.2512486457824707\n",
      "  batch 40 loss: 0.2626010075211525\n",
      "  batch 50 loss: 0.25821406692266463\n",
      "  batch 60 loss: 0.25656700432300567\n",
      "  batch 70 loss: 0.25165138840675355\n",
      "  batch 80 loss: 0.24639963060617448\n",
      "  batch 90 loss: 0.2331764966249466\n",
      "  batch 100 loss: 0.26179208904504775\n",
      "  batch 110 loss: 0.24208825826644897\n",
      "  batch 120 loss: 0.2500067263841629\n",
      "  batch 130 loss: 0.24578554332256317\n",
      "  batch 140 loss: 0.23011959940195084\n",
      "  batch 150 loss: 0.2571588113903999\n",
      "  batch 160 loss: 0.26069304496049883\n",
      "  batch 170 loss: 0.24549576193094252\n",
      "  batch 180 loss: 0.2645336702466011\n",
      "  batch 190 loss: 0.26404208689928055\n",
      "  batch 200 loss: 0.2599194452166557\n",
      "LOSS train 0.2599194452166557 valid 0.24668769538402557\n",
      "ACC train 0.8974130667892157 valid 0.900390625\n",
      "F1 score: 0.8837059913281315\n",
      "EPOCH 45:\n",
      "  batch 10 loss: 0.25655092895030973\n",
      "  batch 20 loss: 0.24331247955560684\n",
      "  batch 30 loss: 0.2434825375676155\n",
      "  batch 40 loss: 0.2487000495195389\n",
      "  batch 50 loss: 0.2434775412082672\n",
      "  batch 60 loss: 0.238331700861454\n",
      "  batch 70 loss: 0.25752065479755404\n",
      "  batch 80 loss: 0.25476862490177155\n",
      "  batch 90 loss: 0.2584387719631195\n",
      "  batch 100 loss: 0.26508081406354905\n",
      "  batch 110 loss: 0.23761566430330278\n",
      "  batch 120 loss: 0.24234297573566438\n",
      "  batch 130 loss: 0.2541585311293602\n",
      "  batch 140 loss: 0.24522994458675385\n",
      "  batch 150 loss: 0.24307278096675872\n",
      "  batch 160 loss: 0.24492916464805603\n",
      "  batch 170 loss: 0.26980913132429124\n",
      "  batch 180 loss: 0.2645071268081665\n",
      "  batch 190 loss: 0.24905447512865067\n",
      "  batch 200 loss: 0.2509609073400497\n",
      "LOSS train 0.2509609073400497 valid 0.2458907514810562\n",
      "ACC train 0.8983992034313726 valid 0.90234375\n",
      "F1 score: 0.8865079963613575\n",
      "EPOCH 46:\n",
      "  batch 10 loss: 0.2618283450603485\n",
      "  batch 20 loss: 0.2555390313267708\n",
      "  batch 30 loss: 0.25701906979084016\n",
      "  batch 40 loss: 0.24168398529291152\n",
      "  batch 50 loss: 0.2541773110628128\n",
      "  batch 60 loss: 0.2580608159303665\n",
      "  batch 70 loss: 0.24506564885377885\n",
      "  batch 80 loss: 0.25255641639232634\n",
      "  batch 90 loss: 0.25674631595611574\n",
      "  batch 100 loss: 0.2511383146047592\n",
      "  batch 110 loss: 0.2468552529811859\n",
      "  batch 120 loss: 0.24547956883907318\n",
      "  batch 130 loss: 0.2523955345153809\n",
      "  batch 140 loss: 0.24950531870126724\n",
      "  batch 150 loss: 0.27280302345752716\n",
      "  batch 160 loss: 0.24406772553920747\n",
      "  batch 170 loss: 0.24551258832216263\n",
      "  batch 180 loss: 0.2498660534620285\n",
      "  batch 190 loss: 0.23963685035705568\n",
      "  batch 200 loss: 0.24075114727020264\n",
      "LOSS train 0.24075114727020264 valid 0.24621975421905518\n",
      "ACC train 0.8978630514705882 valid 0.90625\n",
      "F1 score: 0.8929598615099414\n",
      "EPOCH 47:\n",
      "  batch 10 loss: 0.2505206525325775\n",
      "  batch 20 loss: 0.2526055708527565\n",
      "  batch 30 loss: 0.26432069540023806\n",
      "  batch 40 loss: 0.24488489478826522\n",
      "  batch 50 loss: 0.23252906203269957\n",
      "  batch 60 loss: 0.2529984340071678\n",
      "  batch 70 loss: 0.2532794654369354\n",
      "  batch 80 loss: 0.24093856066465377\n",
      "  batch 90 loss: 0.24566379338502883\n",
      "  batch 100 loss: 0.2382853090763092\n",
      "  batch 110 loss: 0.2452100083231926\n",
      "  batch 120 loss: 0.256850628554821\n",
      "  batch 130 loss: 0.23816849887371064\n",
      "  batch 140 loss: 0.2655882552266121\n",
      "  batch 150 loss: 0.25811543464660647\n",
      "  batch 160 loss: 0.2540101230144501\n",
      "  batch 170 loss: 0.24797109812498092\n",
      "  batch 180 loss: 0.24670947790145875\n",
      "  batch 190 loss: 0.24890777915716172\n",
      "  batch 200 loss: 0.25123409926891327\n",
      "LOSS train 0.25123409926891327 valid 0.24525003135204315\n",
      "ACC train 0.8987342984068627 valid 0.91015625\n",
      "F1 score: 0.8974198672803606\n",
      "EPOCH 48:\n",
      "  batch 10 loss: 0.25354338735342025\n",
      "  batch 20 loss: 0.2399400770664215\n",
      "  batch 30 loss: 0.25188328325748444\n",
      "  batch 40 loss: 0.2504180297255516\n",
      "  batch 50 loss: 0.2418987676501274\n",
      "  batch 60 loss: 0.2452937290072441\n",
      "  batch 70 loss: 0.27625389099121095\n",
      "  batch 80 loss: 0.24147864282131196\n",
      "  batch 90 loss: 0.24629381895065308\n",
      "  batch 100 loss: 0.2368514880537987\n",
      "  batch 110 loss: 0.2579927980899811\n",
      "  batch 120 loss: 0.2505893498659134\n",
      "  batch 130 loss: 0.2466438516974449\n",
      "  batch 140 loss: 0.2369123950600624\n",
      "  batch 150 loss: 0.24929205775260926\n",
      "  batch 160 loss: 0.24007709324359894\n",
      "  batch 170 loss: 0.24500215649604798\n",
      "  batch 180 loss: 0.259418985247612\n",
      "  batch 190 loss: 0.24136582762002945\n",
      "  batch 200 loss: 0.24778103679418564\n",
      "LOSS train 0.24778103679418564 valid 0.2430676966905594\n",
      "ACC train 0.8998927696078431 valid 0.904296875\n",
      "F1 score: 0.8892795844166396\n",
      "EPOCH 49:\n",
      "  batch 10 loss: 0.23853760659694673\n",
      "  batch 20 loss: 0.25076370388269426\n",
      "  batch 30 loss: 0.2543646529316902\n",
      "  batch 40 loss: 0.24137778282165528\n",
      "  batch 50 loss: 0.25519886910915374\n",
      "  batch 60 loss: 0.2553097039461136\n",
      "  batch 70 loss: 0.24267273843288423\n",
      "  batch 80 loss: 0.2352692663669586\n",
      "  batch 90 loss: 0.2521458759903908\n",
      "  batch 100 loss: 0.24572014361619948\n",
      "  batch 110 loss: 0.2435785487294197\n",
      "  batch 120 loss: 0.24125513434410095\n",
      "  batch 130 loss: 0.2504935711622238\n",
      "  batch 140 loss: 0.25559134036302567\n",
      "  batch 150 loss: 0.24817880094051362\n",
      "  batch 160 loss: 0.24790512770414352\n",
      "  batch 170 loss: 0.2360697865486145\n",
      "  batch 180 loss: 0.24122477769851686\n",
      "  batch 190 loss: 0.2582979530096054\n",
      "  batch 200 loss: 0.2437405064702034\n",
      "LOSS train 0.2437405064702034 valid 0.2420709729194641\n",
      "ACC train 0.8995576746323529 valid 0.91015625\n",
      "F1 score: 0.8974198672803606\n",
      "EPOCH 50:\n",
      "  batch 10 loss: 0.2382390543818474\n",
      "  batch 20 loss: 0.25163927525281904\n",
      "  batch 30 loss: 0.23843277394771575\n",
      "  batch 40 loss: 0.2560805484652519\n",
      "  batch 50 loss: 0.2492482379078865\n",
      "  batch 60 loss: 0.25380317866802216\n",
      "  batch 70 loss: 0.24777088612318038\n",
      "  batch 80 loss: 0.2477969780564308\n",
      "  batch 90 loss: 0.25077657103538514\n",
      "  batch 100 loss: 0.23695603460073472\n",
      "  batch 110 loss: 0.23749981224536895\n",
      "  batch 120 loss: 0.24452613592147826\n",
      "  batch 130 loss: 0.2513225257396698\n",
      "  batch 140 loss: 0.23892396986484526\n",
      "  batch 150 loss: 0.25488420128822326\n",
      "  batch 160 loss: 0.25098485201597215\n",
      "  batch 170 loss: 0.25322537273168566\n",
      "  batch 180 loss: 0.2523124128580093\n",
      "  batch 190 loss: 0.23912305235862732\n",
      "  batch 200 loss: 0.24677788615226745\n",
      "LOSS train 0.24677788615226745 valid 0.24095574021339417\n",
      "ACC train 0.8996151194852942 valid 0.91015625\n",
      "F1 score: 0.8974198672803606\n",
      "EPOCH 51:\n",
      "  batch 10 loss: 0.24094227701425552\n",
      "  batch 20 loss: 0.24224051982164382\n",
      "  batch 30 loss: 0.23678845316171646\n",
      "  batch 40 loss: 0.2479114204645157\n",
      "  batch 50 loss: 0.2534386500716209\n",
      "  batch 60 loss: 0.25035165548324584\n",
      "  batch 70 loss: 0.2528118059039116\n",
      "  batch 80 loss: 0.23781557977199555\n",
      "  batch 90 loss: 0.23803309500217437\n",
      "  batch 100 loss: 0.25114299207925794\n",
      "  batch 110 loss: 0.2386200398206711\n",
      "  batch 120 loss: 0.2484402060508728\n",
      "  batch 130 loss: 0.2404315248131752\n",
      "  batch 140 loss: 0.24656938761472702\n",
      "  batch 150 loss: 0.25211824029684066\n",
      "  batch 160 loss: 0.24000345766544343\n",
      "  batch 170 loss: 0.24030883312225343\n",
      "  batch 180 loss: 0.25287546813488004\n",
      "  batch 190 loss: 0.24322399049997329\n",
      "  batch 200 loss: 0.2456730470061302\n",
      "LOSS train 0.2456730470061302 valid 0.24022547900676727\n",
      "ACC train 0.9005438112745098 valid 0.908203125\n",
      "F1 score: 0.8956366867098888\n",
      "EPOCH 52:\n",
      "  batch 10 loss: 0.24824665933847428\n",
      "  batch 20 loss: 0.24756362587213515\n",
      "  batch 30 loss: 0.2444457322359085\n",
      "  batch 40 loss: 0.23347781598567963\n",
      "  batch 50 loss: 0.24801245629787444\n",
      "  batch 60 loss: 0.2439792647957802\n",
      "  batch 70 loss: 0.23793049454689025\n",
      "  batch 80 loss: 0.22325107157230378\n",
      "  batch 90 loss: 0.2671605572104454\n",
      "  batch 100 loss: 0.24715410470962523\n",
      "  batch 110 loss: 0.2571211740374565\n",
      "  batch 120 loss: 0.24208847880363465\n",
      "  batch 130 loss: 0.24729158729314804\n",
      "  batch 140 loss: 0.25333055257797243\n",
      "  batch 150 loss: 0.25213286429643633\n",
      "  batch 160 loss: 0.24258413463830947\n",
      "  batch 170 loss: 0.2417597606778145\n",
      "  batch 180 loss: 0.2250849038362503\n",
      "  batch 190 loss: 0.24647568315267562\n",
      "  batch 200 loss: 0.23371556252241135\n",
      "LOSS train 0.23371556252241135 valid 0.24066713452339172\n",
      "ACC train 0.9001991421568627 valid 0.908203125\n",
      "F1 score: 0.8956366867098888\n",
      "EPOCH 53:\n",
      "  batch 10 loss: 0.24412286430597305\n",
      "  batch 20 loss: 0.23273976743221284\n",
      "  batch 30 loss: 0.24436782151460648\n",
      "  batch 40 loss: 0.2299315646290779\n",
      "  batch 50 loss: 0.24200762510299684\n",
      "  batch 60 loss: 0.23870811760425567\n",
      "  batch 70 loss: 0.23962356001138688\n",
      "  batch 80 loss: 0.23715457916259766\n",
      "  batch 90 loss: 0.2440161734819412\n",
      "  batch 100 loss: 0.2463180750608444\n",
      "  batch 110 loss: 0.2507151588797569\n",
      "  batch 120 loss: 0.2362688511610031\n",
      "  batch 130 loss: 0.2398858979344368\n",
      "  batch 140 loss: 0.2563176989555359\n",
      "  batch 150 loss: 0.24199568182229997\n",
      "  batch 160 loss: 0.24985603243112564\n",
      "  batch 170 loss: 0.24765117913484574\n",
      "  batch 180 loss: 0.24118384718894958\n",
      "  batch 190 loss: 0.24287043660879135\n",
      "  batch 200 loss: 0.2515126600861549\n",
      "LOSS train 0.2515126600861549 valid 0.23816561698913574\n",
      "ACC train 0.9014437806372549 valid 0.9140625\n",
      "F1 score: 0.9027088933691756\n",
      "EPOCH 54:\n",
      "  batch 10 loss: 0.2354120969772339\n",
      "  batch 20 loss: 0.25202943533658984\n",
      "  batch 30 loss: 0.2457856446504593\n",
      "  batch 40 loss: 0.229188035428524\n",
      "  batch 50 loss: 0.23448845893144607\n",
      "  batch 60 loss: 0.2627314865589142\n",
      "  batch 70 loss: 0.2465415596961975\n",
      "  batch 80 loss: 0.2299436330795288\n",
      "  batch 90 loss: 0.22234786599874495\n",
      "  batch 100 loss: 0.25081675201654435\n",
      "  batch 110 loss: 0.24576597213745116\n",
      "  batch 120 loss: 0.23855475932359696\n",
      "  batch 130 loss: 0.24339790046215057\n",
      "  batch 140 loss: 0.2399197295308113\n",
      "  batch 150 loss: 0.23351951837539672\n",
      "  batch 160 loss: 0.25563728958368304\n",
      "  batch 170 loss: 0.24904598593711852\n",
      "  batch 180 loss: 0.24699911922216417\n",
      "  batch 190 loss: 0.24820416271686555\n",
      "  batch 200 loss: 0.2542771503329277\n",
      "LOSS train 0.2542771503329277 valid 0.23714613914489746\n",
      "ACC train 0.9010320925245098 valid 0.90625\n",
      "F1 score: 0.8929598615099414\n",
      "EPOCH 55:\n",
      "  batch 10 loss: 0.22911568284034728\n",
      "  batch 20 loss: 0.2342188388109207\n",
      "  batch 30 loss: 0.23358010202646257\n",
      "  batch 40 loss: 0.24610130339860917\n",
      "  batch 50 loss: 0.23598728477954864\n",
      "  batch 60 loss: 0.25668226927518845\n",
      "  batch 70 loss: 0.23036826997995377\n",
      "  batch 80 loss: 0.23681256771087647\n",
      "  batch 90 loss: 0.2355542540550232\n",
      "  batch 100 loss: 0.24360927790403367\n",
      "  batch 110 loss: 0.25100176930427553\n",
      "  batch 120 loss: 0.25186963081359864\n",
      "  batch 130 loss: 0.232088540494442\n",
      "  batch 140 loss: 0.25224664658308027\n",
      "  batch 150 loss: 0.24928520619869232\n",
      "  batch 160 loss: 0.23299694508314134\n",
      "  batch 170 loss: 0.2439689040184021\n",
      "  batch 180 loss: 0.24170972853899003\n",
      "  batch 190 loss: 0.2446955993771553\n",
      "  batch 200 loss: 0.2370234802365303\n",
      "LOSS train 0.2370234802365303 valid 0.23647713661193848\n",
      "ACC train 0.9016352634803921 valid 0.91015625\n",
      "F1 score: 0.8982865703405016\n",
      "EPOCH 56:\n",
      "  batch 10 loss: 0.24039293080568314\n",
      "  batch 20 loss: 0.2473313629627228\n",
      "  batch 30 loss: 0.2451948642730713\n",
      "  batch 40 loss: 0.23338491320610047\n",
      "  batch 50 loss: 0.24052591174840926\n",
      "  batch 60 loss: 0.22891665548086165\n",
      "  batch 70 loss: 0.23696604520082473\n",
      "  batch 80 loss: 0.234805865585804\n",
      "  batch 90 loss: 0.2441682517528534\n",
      "  batch 100 loss: 0.2384108707308769\n",
      "  batch 110 loss: 0.23617182672023773\n",
      "  batch 120 loss: 0.25145806968212125\n",
      "  batch 130 loss: 0.24356383234262466\n",
      "  batch 140 loss: 0.24293648898601533\n",
      "  batch 150 loss: 0.24760703146457672\n",
      "  batch 160 loss: 0.23824578672647476\n",
      "  batch 170 loss: 0.2325170174241066\n",
      "  batch 180 loss: 0.22416202574968339\n",
      "  batch 190 loss: 0.2518979236483574\n",
      "  batch 200 loss: 0.2552607238292694\n",
      "LOSS train 0.2552607238292694 valid 0.23546868562698364\n",
      "ACC train 0.9017597273284313 valid 0.912109375\n",
      "F1 score: 0.9000776787647871\n",
      "EPOCH 57:\n",
      "  batch 10 loss: 0.23974196016788482\n",
      "  batch 20 loss: 0.23044617623090743\n",
      "  batch 30 loss: 0.2347899332642555\n",
      "  batch 40 loss: 0.23228967040777207\n",
      "  batch 50 loss: 0.25982233285903933\n",
      "  batch 60 loss: 0.23252903968095778\n",
      "  batch 70 loss: 0.23412335067987441\n",
      "  batch 80 loss: 0.24107362627983092\n",
      "  batch 90 loss: 0.23967891335487365\n",
      "  batch 100 loss: 0.2383004441857338\n",
      "  batch 110 loss: 0.2541644960641861\n",
      "  batch 120 loss: 0.2288337305188179\n",
      "  batch 130 loss: 0.23792983442544938\n",
      "  batch 140 loss: 0.22827759236097336\n",
      "  batch 150 loss: 0.24820427745580673\n",
      "  batch 160 loss: 0.2334677904844284\n",
      "  batch 170 loss: 0.23842739164829255\n",
      "  batch 180 loss: 0.24333330243825912\n",
      "  batch 190 loss: 0.24836062043905258\n",
      "  batch 200 loss: 0.2529966622591019\n",
      "LOSS train 0.2529966622591019 valid 0.2358095347881317\n",
      "ACC train 0.9032820159313726 valid 0.91015625\n",
      "F1 score: 0.8974198672803606\n",
      "EPOCH 58:\n",
      "  batch 10 loss: 0.22746429592370987\n",
      "  batch 20 loss: 0.2361129865050316\n",
      "  batch 30 loss: 0.24073193073272706\n",
      "  batch 40 loss: 0.24649960398674012\n",
      "  batch 50 loss: 0.23498752117156982\n",
      "  batch 60 loss: 0.25041806548833845\n",
      "  batch 70 loss: 0.2376013234257698\n",
      "  batch 80 loss: 0.239802086353302\n",
      "  batch 90 loss: 0.23871520459651946\n",
      "  batch 100 loss: 0.2348836824297905\n",
      "  batch 110 loss: 0.2453744515776634\n",
      "  batch 120 loss: 0.2284798800945282\n",
      "  batch 130 loss: 0.23449135571718216\n",
      "  batch 140 loss: 0.24268401116132737\n",
      "  batch 150 loss: 0.24470820128917695\n",
      "  batch 160 loss: 0.22922466546297074\n",
      "  batch 170 loss: 0.23997912257909776\n",
      "  batch 180 loss: 0.2287863478064537\n",
      "  batch 190 loss: 0.24262687414884568\n",
      "  batch 200 loss: 0.23693854063749314\n",
      "LOSS train 0.23693854063749314 valid 0.23350262641906738\n",
      "ACC train 0.9033969056372549 valid 0.91015625\n",
      "F1 score: 0.8974198672803606\n",
      "EPOCH 59:\n",
      "  batch 10 loss: 0.23649970144033433\n",
      "  batch 20 loss: 0.23284329026937484\n",
      "  batch 30 loss: 0.24787391126155853\n",
      "  batch 40 loss: 0.2262320563197136\n",
      "  batch 50 loss: 0.24606252014636992\n",
      "  batch 60 loss: 0.2364198312163353\n",
      "  batch 70 loss: 0.24017090052366258\n",
      "  batch 80 loss: 0.23862538933753968\n",
      "  batch 90 loss: 0.22411923706531525\n",
      "  batch 100 loss: 0.243305067718029\n",
      "  batch 110 loss: 0.23328343629837037\n",
      "  batch 120 loss: 0.23653884083032609\n",
      "  batch 130 loss: 0.25325199365615847\n",
      "  batch 140 loss: 0.22162398993968963\n",
      "  batch 150 loss: 0.24502681642770768\n",
      "  batch 160 loss: 0.2305385559797287\n",
      "  batch 170 loss: 0.24015737175941468\n",
      "  batch 180 loss: 0.2383709281682968\n",
      "  batch 190 loss: 0.23701809495687484\n",
      "  batch 200 loss: 0.24325667470693588\n",
      "LOSS train 0.24325667470693588 valid 0.23277711868286133\n",
      "ACC train 0.9034830729166666 valid 0.912109375\n",
      "F1 score: 0.9000776787647871\n",
      "EPOCH 60:\n",
      "  batch 10 loss: 0.24625724256038667\n",
      "  batch 20 loss: 0.24313360303640366\n",
      "  batch 30 loss: 0.23597535043954848\n",
      "  batch 40 loss: 0.2501195028424263\n",
      "  batch 50 loss: 0.22929693758487701\n",
      "  batch 60 loss: 0.22997514456510543\n",
      "  batch 70 loss: 0.24220191538333893\n",
      "  batch 80 loss: 0.23012130111455917\n",
      "  batch 90 loss: 0.22930710166692733\n",
      "  batch 100 loss: 0.23440496921539306\n",
      "  batch 110 loss: 0.2386411353945732\n",
      "  batch 120 loss: 0.2469379261136055\n",
      "  batch 130 loss: 0.23814007490873337\n",
      "  batch 140 loss: 0.23379511535167694\n",
      "  batch 150 loss: 0.23260990530252457\n",
      "  batch 160 loss: 0.23560807704925538\n",
      "  batch 170 loss: 0.24205549955368041\n",
      "  batch 180 loss: 0.22103665620088578\n",
      "  batch 190 loss: 0.23640812784433365\n",
      "  batch 200 loss: 0.24916547685861587\n",
      "LOSS train 0.24916547685861587 valid 0.23223046958446503\n",
      "ACC train 0.9043064491421569 valid 0.9140625\n",
      "F1 score: 0.9027088933691756\n",
      "EPOCH 61:\n",
      "  batch 10 loss: 0.22988784164190293\n",
      "  batch 20 loss: 0.24606569558382035\n",
      "  batch 30 loss: 0.2321127399802208\n",
      "  batch 40 loss: 0.24193740934133529\n",
      "  batch 50 loss: 0.23724975138902665\n",
      "  batch 60 loss: 0.24512989223003387\n",
      "  batch 70 loss: 0.24209907203912734\n",
      "  batch 80 loss: 0.22766151428222656\n",
      "  batch 90 loss: 0.21763067543506623\n",
      "  batch 100 loss: 0.23545651286840438\n",
      "  batch 110 loss: 0.2347577691078186\n",
      "  batch 120 loss: 0.21870312839746475\n",
      "  batch 130 loss: 0.22890277504920958\n",
      "  batch 140 loss: 0.2393173709511757\n",
      "  batch 150 loss: 0.24329867362976074\n",
      "  batch 160 loss: 0.24034118205308913\n",
      "  batch 170 loss: 0.23720137029886246\n",
      "  batch 180 loss: 0.24105135947465897\n",
      "  batch 190 loss: 0.22514404654502868\n",
      "  batch 200 loss: 0.2348831757903099\n",
      "LOSS train 0.2348831757903099 valid 0.23116366565227509\n",
      "ACC train 0.9046128216911765 valid 0.912109375\n",
      "F1 score: 0.9009102127363736\n",
      "EPOCH 62:\n",
      "  batch 10 loss: 0.2413148835301399\n",
      "  batch 20 loss: 0.23296985924243926\n",
      "  batch 30 loss: 0.2418075606226921\n",
      "  batch 40 loss: 0.22601743787527084\n",
      "  batch 50 loss: 0.2361610397696495\n",
      "  batch 60 loss: 0.23683146387338638\n",
      "  batch 70 loss: 0.23445242196321486\n",
      "  batch 80 loss: 0.23229788839817048\n",
      "  batch 90 loss: 0.2349838137626648\n",
      "  batch 100 loss: 0.239000304043293\n",
      "  batch 110 loss: 0.2257779136300087\n",
      "  batch 120 loss: 0.23492685258388518\n",
      "  batch 130 loss: 0.23533293306827546\n",
      "  batch 140 loss: 0.23388580232858658\n",
      "  batch 150 loss: 0.24608930051326752\n",
      "  batch 160 loss: 0.2324470952153206\n",
      "  batch 170 loss: 0.23135391473770142\n",
      "  batch 180 loss: 0.23095138221979142\n",
      "  batch 190 loss: 0.2397458702325821\n",
      "  batch 200 loss: 0.23319089859724046\n",
      "LOSS train 0.23319089859724046 valid 0.23098351061344147\n",
      "ACC train 0.9050436580882353 valid 0.91015625\n",
      "F1 score: 0.8982865703405016\n",
      "EPOCH 63:\n",
      "  batch 10 loss: 0.2301952376961708\n",
      "  batch 20 loss: 0.23519665449857713\n",
      "  batch 30 loss: 0.24043785631656647\n",
      "  batch 40 loss: 0.23091938048601152\n",
      "  batch 50 loss: 0.2341171219944954\n",
      "  batch 60 loss: 0.24311058968305588\n",
      "  batch 70 loss: 0.2210111901164055\n",
      "  batch 80 loss: 0.23345960080623626\n",
      "  batch 90 loss: 0.25034536570310595\n",
      "  batch 100 loss: 0.23413462340831756\n",
      "  batch 110 loss: 0.21752886325120926\n",
      "  batch 120 loss: 0.24791162461042404\n",
      "  batch 130 loss: 0.22699372768402098\n",
      "  batch 140 loss: 0.23273395150899887\n",
      "  batch 150 loss: 0.23437047153711318\n",
      "  batch 160 loss: 0.23640162199735643\n",
      "  batch 170 loss: 0.2509314253926277\n",
      "  batch 180 loss: 0.22192310839891433\n",
      "  batch 190 loss: 0.22748335897922517\n",
      "  batch 200 loss: 0.22413809448480607\n",
      "LOSS train 0.22413809448480607 valid 0.22922202944755554\n",
      "ACC train 0.9055223651960784 valid 0.91015625\n",
      "F1 score: 0.8974198672803606\n",
      "EPOCH 64:\n",
      "  batch 10 loss: 0.23146368563175201\n",
      "  batch 20 loss: 0.2281055137515068\n",
      "  batch 30 loss: 0.23242829889059066\n",
      "  batch 40 loss: 0.23495610058307648\n",
      "  batch 50 loss: 0.24256374537944794\n",
      "  batch 60 loss: 0.225828155875206\n",
      "  batch 70 loss: 0.24625216871500016\n",
      "  batch 80 loss: 0.22542041391134263\n",
      "  batch 90 loss: 0.2300244078040123\n",
      "  batch 100 loss: 0.21894748508930206\n",
      "  batch 110 loss: 0.23752202540636064\n",
      "  batch 120 loss: 0.22699881047010423\n",
      "  batch 130 loss: 0.22877080589532853\n",
      "  batch 140 loss: 0.2400607645511627\n",
      "  batch 150 loss: 0.2358324036002159\n",
      "  batch 160 loss: 0.24172538369894028\n",
      "  batch 170 loss: 0.23406921178102494\n",
      "  batch 180 loss: 0.23752640783786774\n",
      "  batch 190 loss: 0.23130604475736619\n",
      "  batch 200 loss: 0.22975070178508758\n",
      "LOSS train 0.22975070178508758 valid 0.2291800081729889\n",
      "ACC train 0.9045553768382353 valid 0.9140625\n",
      "F1 score: 0.9027088933691756\n",
      "EPOCH 65:\n",
      "  batch 10 loss: 0.21820831894874573\n",
      "  batch 20 loss: 0.24309155195951462\n",
      "  batch 30 loss: 0.2186383858323097\n",
      "  batch 40 loss: 0.2252015709877014\n",
      "  batch 50 loss: 0.233650304377079\n",
      "  batch 60 loss: 0.2169372096657753\n",
      "  batch 70 loss: 0.235096575319767\n",
      "  batch 80 loss: 0.23220691680908204\n",
      "  batch 90 loss: 0.23413887321949006\n",
      "  batch 100 loss: 0.24791483134031295\n",
      "  batch 110 loss: 0.22099227756261824\n",
      "  batch 120 loss: 0.233580382168293\n",
      "  batch 130 loss: 0.23163026720285415\n",
      "  batch 140 loss: 0.2316026881337166\n",
      "  batch 150 loss: 0.22715043425559997\n",
      "  batch 160 loss: 0.24939523190259932\n",
      "  batch 170 loss: 0.23447272628545762\n",
      "  batch 180 loss: 0.22183284908533096\n",
      "  batch 190 loss: 0.2337319627404213\n",
      "  batch 200 loss: 0.23757005035877227\n",
      "LOSS train 0.23757005035877227 valid 0.2278801053762436\n",
      "ACC train 0.9069106158088235 valid 0.912109375\n",
      "F1 score: 0.9009102127363736\n",
      "EPOCH 66:\n",
      "  batch 10 loss: 0.2321666121482849\n",
      "  batch 20 loss: 0.23302174657583236\n",
      "  batch 30 loss: 0.22425695955753328\n",
      "  batch 40 loss: 0.2191540390253067\n",
      "  batch 50 loss: 0.24754648506641388\n",
      "  batch 60 loss: 0.22616818249225618\n",
      "  batch 70 loss: 0.22652907520532609\n",
      "  batch 80 loss: 0.21559047251939772\n",
      "  batch 90 loss: 0.2319873943924904\n",
      "  batch 100 loss: 0.23916599601507188\n",
      "  batch 110 loss: 0.24728662818670272\n",
      "  batch 120 loss: 0.23076971173286437\n",
      "  batch 130 loss: 0.22684705257415771\n",
      "  batch 140 loss: 0.22496745139360427\n",
      "  batch 150 loss: 0.24116000682115554\n",
      "  batch 160 loss: 0.22995248883962632\n",
      "  batch 170 loss: 0.2385081335902214\n",
      "  batch 180 loss: 0.22801689207553863\n",
      "  batch 190 loss: 0.22835446149110794\n",
      "  batch 200 loss: 0.2299238696694374\n",
      "LOSS train 0.2299238696694374 valid 0.22594772279262543\n",
      "ACC train 0.9063936121323529 valid 0.9140625\n",
      "F1 score: 0.9035082922367165\n",
      "EPOCH 67:\n",
      "  batch 10 loss: 0.22893624305725097\n",
      "  batch 20 loss: 0.2239950641989708\n",
      "  batch 30 loss: 0.23111947476863862\n",
      "  batch 40 loss: 0.23427412509918213\n",
      "  batch 50 loss: 0.2357563152909279\n",
      "  batch 60 loss: 0.23294173181056976\n",
      "  batch 70 loss: 0.22690124362707137\n",
      "  batch 80 loss: 0.23762078881263732\n",
      "  batch 90 loss: 0.22199649810791017\n",
      "  batch 100 loss: 0.23892536014318466\n",
      "  batch 110 loss: 0.22354217916727065\n",
      "  batch 120 loss: 0.21766040623188018\n",
      "  batch 130 loss: 0.22827883064746857\n",
      "  batch 140 loss: 0.21604708582162857\n",
      "  batch 150 loss: 0.2343805581331253\n",
      "  batch 160 loss: 0.22967085540294646\n",
      "  batch 170 loss: 0.23450847417116166\n",
      "  batch 180 loss: 0.22945284992456436\n",
      "  batch 190 loss: 0.23096708506345748\n",
      "  batch 200 loss: 0.23835447579622268\n",
      "LOSS train 0.23835447579622268 valid 0.22555416822433472\n",
      "ACC train 0.9077244178921569 valid 0.916015625\n",
      "F1 score: 0.9053142032814238\n",
      "EPOCH 68:\n",
      "  batch 10 loss: 0.22683510780334473\n",
      "  batch 20 loss: 0.22284179776906968\n",
      "  batch 30 loss: 0.235520800948143\n",
      "  batch 40 loss: 0.22672865837812423\n",
      "  batch 50 loss: 0.23306256383657456\n",
      "  batch 60 loss: 0.21725839972496033\n",
      "  batch 70 loss: 0.2194221019744873\n",
      "  batch 80 loss: 0.2544340640306473\n",
      "  batch 90 loss: 0.23618320971727372\n",
      "  batch 100 loss: 0.22621624022722245\n",
      "  batch 110 loss: 0.2326810136437416\n",
      "  batch 120 loss: 0.23703148514032363\n",
      "  batch 130 loss: 0.2281273990869522\n",
      "  batch 140 loss: 0.2303786724805832\n",
      "  batch 150 loss: 0.23522451072931289\n",
      "  batch 160 loss: 0.22782902419567108\n",
      "  batch 170 loss: 0.22803687006235124\n",
      "  batch 180 loss: 0.22131988555192947\n",
      "  batch 190 loss: 0.22672852426767348\n",
      "  batch 200 loss: 0.24859000891447067\n",
      "LOSS train 0.24859000891447067 valid 0.223959282040596\n",
      "ACC train 0.9065467984068627 valid 0.919921875\n",
      "F1 score: 0.9097181938264739\n",
      "EPOCH 69:\n",
      "  batch 10 loss: 0.22660867422819136\n",
      "  batch 20 loss: 0.21888581663370132\n",
      "  batch 30 loss: 0.21028816998004912\n",
      "  batch 40 loss: 0.22530827075242996\n",
      "  batch 50 loss: 0.23431134819984437\n",
      "  batch 60 loss: 0.22731995284557344\n",
      "  batch 70 loss: 0.23055134117603301\n",
      "  batch 80 loss: 0.21684558242559432\n",
      "  batch 90 loss: 0.21488404273986816\n",
      "  batch 100 loss: 0.22827693223953247\n",
      "  batch 110 loss: 0.22751926332712175\n",
      "  batch 120 loss: 0.23419589251279832\n",
      "  batch 130 loss: 0.23179097026586531\n",
      "  batch 140 loss: 0.2320726364850998\n",
      "  batch 150 loss: 0.22806136459112167\n",
      "  batch 160 loss: 0.23661684542894362\n",
      "  batch 170 loss: 0.22645475417375566\n",
      "  batch 180 loss: 0.22943725138902665\n",
      "  batch 190 loss: 0.23906754702329636\n",
      "  batch 200 loss: 0.2237743243575096\n",
      "LOSS train 0.2237743243575096 valid 0.22460055351257324\n",
      "ACC train 0.9080882352941176 valid 0.916015625\n",
      "F1 score: 0.9060814660526338\n",
      "EPOCH 70:\n",
      "  batch 10 loss: 0.22568244785070418\n",
      "  batch 20 loss: 0.24167050272226334\n",
      "  batch 30 loss: 0.22167782187461854\n",
      "  batch 40 loss: 0.22469922453165053\n",
      "  batch 50 loss: 0.22264724969863892\n",
      "  batch 60 loss: 0.22460758984088897\n",
      "  batch 70 loss: 0.2329213872551918\n",
      "  batch 80 loss: 0.23102693110704423\n",
      "  batch 90 loss: 0.2153816342353821\n",
      "  batch 100 loss: 0.2223552107810974\n",
      "  batch 110 loss: 0.23384148478507996\n",
      "  batch 120 loss: 0.24442051500082015\n",
      "  batch 130 loss: 0.23461406975984572\n",
      "  batch 140 loss: 0.22320952266454697\n",
      "  batch 150 loss: 0.2143770918250084\n",
      "  batch 160 loss: 0.2291508361697197\n",
      "  batch 170 loss: 0.23317037522792816\n",
      "  batch 180 loss: 0.2203850269317627\n",
      "  batch 190 loss: 0.2362304151058197\n",
      "  batch 200 loss: 0.22676825523376465\n",
      "LOSS train 0.22676825523376465 valid 0.22285149991512299\n",
      "ACC train 0.9085286458333334 valid 0.912109375\n",
      "F1 score: 0.9000776787647871\n",
      "EPOCH 71:\n",
      "  batch 10 loss: 0.22762944251298906\n",
      "  batch 20 loss: 0.22525323927402496\n",
      "  batch 30 loss: 0.23127119094133378\n",
      "  batch 40 loss: 0.23702237755060196\n",
      "  batch 50 loss: 0.22487963736057281\n",
      "  batch 60 loss: 0.21745857298374177\n",
      "  batch 70 loss: 0.22077310234308242\n",
      "  batch 80 loss: 0.2170696809887886\n",
      "  batch 90 loss: 0.2363142415881157\n",
      "  batch 100 loss: 0.22797616869211196\n",
      "  batch 110 loss: 0.21155813634395598\n",
      "  batch 120 loss: 0.23486699610948564\n",
      "  batch 130 loss: 0.22705693691968917\n",
      "  batch 140 loss: 0.2193464532494545\n",
      "  batch 150 loss: 0.2165420949459076\n",
      "  batch 160 loss: 0.22099625319242477\n",
      "  batch 170 loss: 0.2214595466852188\n",
      "  batch 180 loss: 0.2215507969260216\n",
      "  batch 190 loss: 0.22855528742074965\n",
      "  batch 200 loss: 0.22974358797073363\n",
      "LOSS train 0.22974358797073363 valid 0.2229687124490738\n",
      "ACC train 0.9102424172794118 valid 0.91015625\n",
      "F1 score: 0.8991223055202037\n",
      "EPOCH 72:\n",
      "  batch 10 loss: 0.23163310140371324\n",
      "  batch 20 loss: 0.2261031061410904\n",
      "  batch 30 loss: 0.2281362920999527\n",
      "  batch 40 loss: 0.21629038602113723\n",
      "  batch 50 loss: 0.22095726579427719\n",
      "  batch 60 loss: 0.22854238599538804\n",
      "  batch 70 loss: 0.232635235786438\n",
      "  batch 80 loss: 0.24500248879194259\n",
      "  batch 90 loss: 0.22791633903980255\n",
      "  batch 100 loss: 0.23513525128364562\n",
      "  batch 110 loss: 0.22244084626436234\n",
      "  batch 120 loss: 0.22484831064939498\n",
      "  batch 130 loss: 0.23213042467832565\n",
      "  batch 140 loss: 0.21355156749486923\n",
      "  batch 150 loss: 0.22213547974824904\n",
      "  batch 160 loss: 0.2265825316309929\n",
      "  batch 170 loss: 0.2217549741268158\n",
      "  batch 180 loss: 0.21971511393785476\n",
      "  batch 190 loss: 0.2118559315800667\n",
      "  batch 200 loss: 0.2273364394903183\n",
      "LOSS train 0.2273364394903183 valid 0.22010153532028198\n",
      "ACC train 0.9090265012254902 valid 0.91796875\n",
      "F1 score: 0.90863037109375\n",
      "EPOCH 73:\n",
      "  batch 10 loss: 0.2297751411795616\n",
      "  batch 20 loss: 0.22019775360822677\n",
      "  batch 30 loss: 0.23184740096330642\n",
      "  batch 40 loss: 0.22981918156147002\n",
      "  batch 50 loss: 0.21543246358633042\n",
      "  batch 60 loss: 0.23001568019390106\n",
      "  batch 70 loss: 0.21687478870153426\n",
      "  batch 80 loss: 0.21984369456768035\n",
      "  batch 90 loss: 0.21942381262779237\n",
      "  batch 100 loss: 0.22824214398860931\n",
      "  batch 110 loss: 0.224287948012352\n",
      "  batch 120 loss: 0.22881381213665009\n",
      "  batch 130 loss: 0.21437363624572753\n",
      "  batch 140 loss: 0.2298689916729927\n",
      "  batch 150 loss: 0.21933573335409165\n",
      "  batch 160 loss: 0.20965847373008728\n",
      "  batch 170 loss: 0.2233957275748253\n",
      "  batch 180 loss: 0.22749286741018296\n",
      "  batch 190 loss: 0.23441938310861588\n",
      "  batch 200 loss: 0.21455391645431518\n",
      "LOSS train 0.21455391645431518 valid 0.22074441611766815\n",
      "ACC train 0.9105966605392157 valid 0.91796875\n",
      "F1 score: 0.90863037109375\n",
      "EPOCH 74:\n",
      "  batch 10 loss: 0.21264751702547074\n",
      "  batch 20 loss: 0.2179289996623993\n",
      "  batch 30 loss: 0.21813859641551972\n",
      "  batch 40 loss: 0.23185185194015503\n",
      "  batch 50 loss: 0.2230811059474945\n",
      "  batch 60 loss: 0.22349866032600402\n",
      "  batch 70 loss: 0.22954635322093964\n",
      "  batch 80 loss: 0.2190030261874199\n",
      "  batch 90 loss: 0.22301343083381653\n",
      "  batch 100 loss: 0.22945216596126555\n",
      "  batch 110 loss: 0.22845829129219056\n",
      "  batch 120 loss: 0.2151337221264839\n",
      "  batch 130 loss: 0.2252738133072853\n",
      "  batch 140 loss: 0.21088556945323944\n",
      "  batch 150 loss: 0.23262501657009124\n",
      "  batch 160 loss: 0.2287638306617737\n",
      "  batch 170 loss: 0.22422959953546523\n",
      "  batch 180 loss: 0.22638292461633683\n",
      "  batch 190 loss: 0.21621455252170563\n",
      "  batch 200 loss: 0.22250328958034515\n",
      "LOSS train 0.22250328958034515 valid 0.220326229929924\n",
      "ACC train 0.9109892003676471 valid 0.91796875\n",
      "F1 score: 0.9093407169592153\n",
      "EPOCH 75:\n",
      "  batch 10 loss: 0.22812019735574723\n",
      "  batch 20 loss: 0.22707537412643433\n",
      "  batch 30 loss: 0.21346380710601806\n",
      "  batch 40 loss: 0.22912542074918746\n",
      "  batch 50 loss: 0.23937662243843078\n",
      "  batch 60 loss: 0.22084730714559556\n",
      "  batch 70 loss: 0.22208216935396194\n",
      "  batch 80 loss: 0.20789825469255446\n",
      "  batch 90 loss: 0.22642763704061508\n",
      "  batch 100 loss: 0.2415830001235008\n",
      "  batch 110 loss: 0.21498370319604873\n",
      "  batch 120 loss: 0.2159909725189209\n",
      "  batch 130 loss: 0.2112514480948448\n",
      "  batch 140 loss: 0.21246391236782075\n",
      "  batch 150 loss: 0.22631094455718995\n",
      "  batch 160 loss: 0.21090517044067383\n",
      "  batch 170 loss: 0.22695818692445754\n",
      "  batch 180 loss: 0.21213357746601105\n",
      "  batch 190 loss: 0.22095327973365783\n",
      "  batch 200 loss: 0.23122218549251555\n",
      "LOSS train 0.23122218549251555 valid 0.21896585822105408\n",
      "ACC train 0.9116976868872549 valid 0.916015625\n",
      "F1 score: 0.9068217527932961\n",
      "EPOCH 76:\n",
      "  batch 10 loss: 0.21759833842515947\n",
      "  batch 20 loss: 0.21926749050617217\n",
      "  batch 30 loss: 0.22075122445821763\n",
      "  batch 40 loss: 0.21150737553834914\n",
      "  batch 50 loss: 0.21948185861110686\n",
      "  batch 60 loss: 0.22656001895666122\n",
      "  batch 70 loss: 0.21937331557273865\n",
      "  batch 80 loss: 0.22622381448745726\n",
      "  batch 90 loss: 0.23096076101064683\n",
      "  batch 100 loss: 0.20103846490383148\n",
      "  batch 110 loss: 0.2262527599930763\n",
      "  batch 120 loss: 0.22977459132671357\n",
      "  batch 130 loss: 0.2226010262966156\n",
      "  batch 140 loss: 0.23727215081453323\n",
      "  batch 150 loss: 0.22005940079689026\n",
      "  batch 160 loss: 0.22372739464044572\n",
      "  batch 170 loss: 0.21450768858194352\n",
      "  batch 180 loss: 0.2180341213941574\n",
      "  batch 190 loss: 0.22255548536777497\n",
      "  batch 200 loss: 0.22623991519212722\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.79 GiB total capacity; 7.00 GiB already allocated; 30.31 MiB free; 7.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m vinputs, vlabels \u001b[38;5;241m=\u001b[39m vdata\n\u001b[1;32m     26\u001b[0m vinputs, vlabels \u001b[38;5;241m=\u001b[39m vinputs\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat(), vlabels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 27\u001b[0m voutputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvinputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m vloss \u001b[38;5;241m=\u001b[39m loss_fn(voutputs, vlabels)\n\u001b[1;32m     31\u001b[0m running_vloss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m vloss\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mAlteredNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 14\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/resnet.py:275\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[1;32m    274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n\u001b[0;32m--> 275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n\u001b[1;32m    278\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/resnet.py:93\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m identity \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     92\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m---> 93\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[1;32m     96\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2451\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2452\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 7.79 GiB total capacity; 7.00 GiB already allocated; 30.31 MiB free; 7.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "epoch_number = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    accurate_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    vaccp = 0\n",
    "    vtotp = 0\n",
    "    vatl = np.array([])\n",
    "    vapl = np.array([]) \n",
    "\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss, accuracy = train_one_epoch(epoch_number, accurate_predictions, total_predictions)\n",
    "    #'''\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    for i, vdata in enumerate(test_loader):\n",
    "        vinputs, vlabels = vdata\n",
    "\n",
    "        vinputs, vlabels = vinputs.to(device).float(), vlabels.to(device).float()\n",
    "        voutputs = model(vinputs)\n",
    "        \n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "\n",
    "        running_vloss += vloss\n",
    "        \n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "\n",
    "    voutputs = model.sigmoid_layer(voutputs)\n",
    "\n",
    "    vpredictions = np.round(voutputs.detach().cpu().numpy()).flatten()\n",
    "    #vpredictions = np.where(voutputs.detach().cpu().numpy() > 0.5, 1, 0)  # Apply thresholding for multi-label classificationv\n",
    "    vtarget = vlabels.detach().cpu().numpy().flatten()\n",
    "    for index, vprediction in enumerate(vpredictions):\n",
    "        if vprediction == vtarget[index]:\n",
    "            vaccp += 1\n",
    "        vtotp += 1\n",
    "    vacc = vaccp/vtotp\n",
    "\n",
    "    vatl = np.append(vatl, vtarget)\n",
    "    vapl = np.append(vapl, vpredictions)\n",
    "\n",
    "    f1 = f1_score(vatl, vapl, average='weighted')\n",
    "\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "    print('ACC train {} valid {}'.format(accuracy, vacc))\n",
    "    print('F1 score: {}'.format(f1))\n",
    "\n",
    "    #wandb.log({ 'Training' : avg_loss, 'Validation' : avg_vloss, 'Train Acc' : accuracy, 'Val Acc' : vacc, 'F1 Score': f1})\n",
    "    #'''\n",
    "    #print('ACC train {}'.format(accuracy))\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    epoch_number += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "all_true_labels = np.array([])\n",
    "all_predictions = np.array([])\n",
    "all_true_labels_itemized = np.zeros([1,128])\n",
    "all_predictions_itemized = np.zeros([1,128])\n",
    "\n",
    "running_vloss = 0.0\n",
    "with torch.no_grad():\n",
    "    for i, (data, target) in enumerate(test_loader):\n",
    "        data, target = data.to(device).float(), target.to(device).float()\n",
    "        output = model(data)\n",
    "        \n",
    "        vloss = loss_fn(output, target)\n",
    "        running_vloss += vloss\n",
    "\n",
    "        output = model.sigmoid_layer(output)\n",
    "\n",
    "        predictions = np.where(output.detach().cpu().numpy() > 0.5, 1, 0)  # Apply thresholding for multi-label classification\n",
    "        labels = target.detach().cpu().numpy()\n",
    "        \n",
    "        all_true_labels = np.append(all_true_labels, labels)\n",
    "        all_predictions = np.append(all_predictions, predictions)\n",
    "        \n",
    "        predictions = predictions.transpose()\n",
    "\n",
    "        all_true_labels_itemized = np.concatenate((all_true_labels_itemized, labels), axis=0)\n",
    "        all_predictions_itemized = np.concatenate((all_predictions_itemized, predictions.T), axis=0)\n",
    "\n",
    "avg_vloss = running_vloss / (i + 1)        \n",
    "for index, prediction in enumerate(all_predictions):\n",
    "    if prediction == all_true_labels[index]:\n",
    "        correct += 1\n",
    "print('Accuracy: ', correct/len(all_predictions))\n",
    "print('Loss: ', avg_vloss)\n",
    "print('Confusion Matrix: ', multilabel_confusion_matrix(all_true_labels, all_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_labels = all_true_labels_itemized.transpose()\n",
    "transposed_predictions = all_predictions_itemized.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([\"Adam's apple\", 'almond', 'arched (v-shaped)', 'bags under eyes', 'bald', 'bangs', 'beard', 'big', 'big/wide', 'black', 'blond', \n",
    "                     'buck', 'bulbous', 'bushy', 'button', 'cheekbones', 'cheeks', 'chin', 'chubby/full', 'cleft', 'crooked', 'crows feet', 'curly', \n",
    "                     'curved down', 'deep-set', 'dimples', 'dorsal hump', 'double chin', 'downturned', 'dreads', 'drooping', 'ears', 'eyebrows', 'eyelids', \n",
    "                     'eyes', 'facial hair', 'far apart', 'flared nostrils', 'flat', 'forehead', 'forward', 'freckles', 'furrowed', 'gap', 'glasses', \n",
    "                     'goatee', 'hair', 'handlebar', 'hat', 'head', 'high', 'hooded', 'hooked', 'large', 'lazy eye', 'light', 'light-colored', 'lines', \n",
    "                     'lips', 'long', 'long eyelashes', 'low', 'medial cleft', 'messy', 'mole', 'mouth', 'mustache', 'narrow', 'narrow-set', 'neck', 'nose', \n",
    "                     'overbite', 'pale', 'pierced', 'pointed', 'pointy', 'pouty/full', 'puffy', 'receded', 'receding hairline', 'red', 'red lipstick', \n",
    "                     'rough', 'round', 'rounded', 'rounded tip', 'scar', 'sharp', 'short', 'sideburns', 'skin', 'slanted down', 'slanted up', \n",
    "                     'slicked back', 'small', 'small nostrils', 'smooth', 'soul patch', 'square', 'stick out', 'straight', 'strong jawline', 'stubble', \n",
    "                     'tattoos', 'teeth', 'thick', 'thick lower', 'thin', 'thin bridge', 'thin upper', 'thin/hollow', 'trimmed', 'unibrow', 'upper lip', \n",
    "                     'upturned', 'v-shaped', 'weak jawline', 'well-defined tip', 'white', 'white streaks', 'wide', 'wide bridge', 'wide nostrils', \n",
    "                     'wide tip', 'wide-set', 'wide-x', 'widows peak', 'wrinkled'])\n",
    "\n",
    "'''features = np.array([\n",
    "    'cheekbones', \n",
    "    'cheeks', \n",
    "    'chin', \n",
    "    'ears', \n",
    "    'eyebrows', \n",
    "    'eyelids', \n",
    "    'eyes', \n",
    "    'facial hair', \n",
    "    'forehead', \n",
    "    'hair', \n",
    "    'head', \n",
    "    'lips', \n",
    "    'mouth', \n",
    "    'neck', \n",
    "    'nose', \n",
    "    'skin', \n",
    "    'teeth', \n",
    "    'upper lip'\n",
    "])'''\n",
    "\n",
    "fig, axes = plt.subplots(nrows=64, ncols=2, figsize=(15, 150))\n",
    "\n",
    "for index, (feature, ax) in enumerate(zip(features, axes.flatten())):\n",
    "    cm = confusion_matrix(transposed_labels[index],\n",
    "                         transposed_predictions[index])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(ax=ax)\n",
    "    ax.title.set_text(feature)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nose', 'ears', 'eyebrows', 'facial hair', 'eyes', 'eyelids'], ['nose', 'mouth', 'eyes', 'ears', 'facial hair', 'head', 'chin'], ['head', 'nose', 'ears', 'teeth']]\n",
      "[[['nose_well-defined tip'], ['ears_stick out', 'ears_pierced'], ['eyebrows_arched (v-shaped)'], ['facial hair_mustache', 'facial hair_goatee'], ['eyes_bags under eyes'], ['eyelids_puffy']], [['nose_dorsal hump', 'nose_long'], ['mouth_big/wide'], ['eyes_slanted down'], ['ears_stick out', 'ears_low'], ['facial hair_stubble', 'facial hair_goatee'], ['head_long'], ['chin_weak jawline', 'chin_crooked']], [['head_long'], ['nose_dorsal hump'], ['ears_stick out', 'ears_big'], ['teeth_big']]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# iterate through a file and add the labels to a list\n",
    "data = np.array([line.strip().split('||') for line in open('./labels/combo-labels-extended.txt')], dtype = list)\n",
    "\n",
    "prominentFeatures = data[:,:-1]\n",
    "\n",
    "prom_features = []\n",
    "sub_features = []\n",
    "\n",
    "# iterate through the list and split the labels into lists\n",
    "for i in range(len(data)):\n",
    "    data[i][1] = data[i][1].split('|')\n",
    "    data[i][1] = [s.strip() for s in data[i][1]]\n",
    "    prom_features.append(data[i][1])\n",
    "    data[i][2] = data[i][2].split('|')\n",
    "    temp_sub_list = []\n",
    "    for j in range(len(data[i][2]) - 1):\n",
    "        data[i][2][j] = data[i][2][j].split(',')\n",
    "        for k in range(len(data[i][2][j])):\n",
    "            data[i][2][j][k] = data[i][1][j].strip() + \"_\" + data[i][2][j][k].strip()\n",
    "        temp_sub_list.append(data[i][2][j])\n",
    "    sub_features.append(temp_sub_list)\n",
    "\n",
    "#np.savetxt('./testlabels.txt', data, delimiter = ',', fmt = '%s')\n",
    "print(prom_features[:3])\n",
    "print(sub_features[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_of_possible_prom_features = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    for feature in data[i][1]:\n",
    "        if not feature in array_of_possible_prom_features:\n",
    "            array_of_possible_prom_features.append(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cheekbones', 'cheeks', 'chin', 'ears', 'eyebrows', 'eyelids', 'eyes', 'facial hair', 'forehead', 'hair', 'head', 'lips', 'mouth', 'neck', 'nose', 'skin', 'teeth']\n",
      "17\n",
      "229\n"
     ]
    }
   ],
   "source": [
    "array_of_possible_prom_features.sort()\n",
    "print(array_of_possible_prom_features)\n",
    "print(len(array_of_possible_prom_features))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data = np.empty((len(data), 2), dtype='O')\n",
    "\n",
    "# Iterate through each person, generate a list of 0s, if the feature is present put a 1 in the corresponding index\n",
    "for i in range(len(data)): #iterate through each person\n",
    "    present_features = np.zeros(len(array_of_possible_prom_features))\n",
    "    for feature in data[i][1]:\n",
    "        present_features[array_of_possible_prom_features.index(feature)] = 1\n",
    "    binary_data[i][0] = data[i][0].strip()\n",
    "    binary_data[i][1] = present_features\n",
    "\n",
    "#np.savetxt('binary_labels.txt', binary_data, delimiter = ',', fmt = '%s')\n",
    "#Now the labels are saved \n",
    "#loadtest = np.loadtxt('binary_labels.txt', dtype = 'O', delimiter = ',')\n",
    "# reads in correclty!\n",
    "labels_itemized = pd.DataFrame(binary_data)\n",
    "out_df = pd.DataFrame(labels_itemized.iloc[:, 1].tolist(), index = labels_itemized.iloc[:, 0])\n",
    "out_df = out_df.set_axis(array_of_possible_prom_features, axis=1)\n",
    "out_df.to_csv('./labels/binary_prom_labels_extended.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cheekbones</th>\n",
       "      <th>cheeks</th>\n",
       "      <th>chin</th>\n",
       "      <th>ears</th>\n",
       "      <th>eyebrows</th>\n",
       "      <th>eyelids</th>\n",
       "      <th>eyes</th>\n",
       "      <th>facial hair</th>\n",
       "      <th>forehead</th>\n",
       "      <th>hair</th>\n",
       "      <th>head</th>\n",
       "      <th>lips</th>\n",
       "      <th>mouth</th>\n",
       "      <th>neck</th>\n",
       "      <th>nose</th>\n",
       "      <th>skin</th>\n",
       "      <th>teeth</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aamir_khan</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adam_driver</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adam_sandler</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adele</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aishwarya_rai</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woody_allen</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xi_jinping</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yao_ming</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yo-yo_ma</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoe_saldana</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>229 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               cheekbones  cheeks  chin  ears  eyebrows  eyelids  eyes   \n",
       "0                                                                        \n",
       "aamir_khan            0.0     0.0   0.0   1.0       1.0      1.0   1.0  \\\n",
       "adam_driver           0.0     0.0   1.0   1.0       0.0      0.0   1.0   \n",
       "adam_sandler          0.0     0.0   0.0   1.0       0.0      0.0   0.0   \n",
       "adele                 1.0     1.0   1.0   0.0       0.0      0.0   1.0   \n",
       "aishwarya_rai         0.0     0.0   0.0   0.0       1.0      0.0   1.0   \n",
       "...                   ...     ...   ...   ...       ...      ...   ...   \n",
       "woody_allen           0.0     0.0   1.0   1.0       0.0      1.0   1.0   \n",
       "xi_jinping            0.0     1.0   1.0   0.0       1.0      1.0   1.0   \n",
       "yao_ming              1.0     0.0   1.0   1.0       1.0      0.0   1.0   \n",
       "yo-yo_ma              0.0     1.0   1.0   0.0       1.0      1.0   1.0   \n",
       "zoe_saldana           1.0     0.0   1.0   0.0       1.0      0.0   1.0   \n",
       "\n",
       "               facial hair  forehead  hair  head  lips  mouth  neck  nose   \n",
       "0                                                                           \n",
       "aamir_khan             1.0       0.0   0.0   0.0   0.0    0.0   0.0   1.0  \\\n",
       "adam_driver            1.0       0.0   0.0   1.0   0.0    1.0   0.0   1.0   \n",
       "adam_sandler           0.0       0.0   0.0   1.0   0.0    0.0   0.0   1.0   \n",
       "adele                  0.0       0.0   0.0   1.0   0.0    0.0   0.0   1.0   \n",
       "aishwarya_rai          0.0       0.0   0.0   1.0   1.0    0.0   0.0   1.0   \n",
       "...                    ...       ...   ...   ...   ...    ...   ...   ...   \n",
       "woody_allen            0.0       1.0   1.0   0.0   1.0    0.0   0.0   1.0   \n",
       "xi_jinping             0.0       1.0   1.0   0.0   1.0    0.0   0.0   1.0   \n",
       "yao_ming               0.0       1.0   0.0   0.0   1.0    0.0   0.0   1.0   \n",
       "yo-yo_ma               0.0       0.0   1.0   1.0   1.0    0.0   0.0   1.0   \n",
       "zoe_saldana            0.0       1.0   0.0   0.0   0.0    1.0   0.0   1.0   \n",
       "\n",
       "               skin  teeth  \n",
       "0                           \n",
       "aamir_khan      0.0    0.0  \n",
       "adam_driver     0.0    0.0  \n",
       "adam_sandler    0.0    1.0  \n",
       "adele           0.0    0.0  \n",
       "aishwarya_rai   0.0    0.0  \n",
       "...             ...    ...  \n",
       "woody_allen     0.0    0.0  \n",
       "xi_jinping      0.0    0.0  \n",
       "yao_ming        0.0    1.0  \n",
       "yo-yo_ma        0.0    0.0  \n",
       "zoe_saldana     1.0    0.0  \n",
       "\n",
       "[229 rows x 17 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_of_possible_sub_features = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(sub_features[i])):\n",
    "        for sub in sub_features[i][j]:\n",
    "            if not sub in array_of_possible_sub_features:\n",
    "                array_of_possible_sub_features.append(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cheekbones_high', 'cheekbones_sharp', 'cheeks_chubby/full', 'cheeks_dimples', 'cheeks_thin/hollow', 'chin_cleft', 'chin_crooked', 'chin_double chin', 'chin_forward', 'chin_pointed', 'chin_rounded', 'chin_scar', 'chin_square', 'chin_strong jawline', 'chin_weak jawline', 'ears_big', 'ears_flat', 'ears_high', 'ears_low', 'ears_pierced', 'ears_pointy', 'ears_small', 'ears_stick out', 'eyebrows_arched (v-shaped)', 'eyebrows_bushy', 'eyebrows_curved down', 'eyebrows_far apart', 'eyebrows_flat', 'eyebrows_furrowed', 'eyebrows_light', 'eyebrows_long', 'eyebrows_scar', 'eyebrows_short', 'eyebrows_slanted down', 'eyebrows_thick', 'eyebrows_thin', 'eyebrows_unibrow', 'eyelids_drooping', 'eyelids_hooded', 'eyelids_puffy', 'eyelids_receded', 'eyes_almond', 'eyes_bags under eyes', 'eyes_crows feet', 'eyes_deep-set', 'eyes_glasses', 'eyes_lazy eye', 'eyes_light-colored', 'eyes_long eyelashes', 'eyes_narrow', 'eyes_narrow-set', 'eyes_round', 'eyes_slanted down', 'eyes_slanted up', 'eyes_small', 'eyes_stick out', 'eyes_wide', 'eyes_wide-set', 'eyes_wide-x', 'facial hair_beard', 'facial hair_goatee', 'facial hair_handlebar', 'facial hair_messy', 'facial hair_mustache', 'facial hair_sideburns', 'facial hair_soul patch', 'facial hair_stubble', 'facial hair_thick', 'facial hair_thin', 'facial hair_trimmed', 'facial hair_white', 'forehead_big', 'forehead_narrow', 'forehead_scar', 'forehead_small', 'forehead_wide', 'forehead_wrinkled', 'hair_bald', 'hair_bangs', 'hair_big', 'hair_black', 'hair_blond', 'hair_curly', 'hair_dreads', 'hair_hat', 'hair_long', 'hair_receding hairline', 'hair_red', 'hair_short', 'hair_slicked back', 'hair_white', 'hair_white streaks', 'hair_widows peak', 'head_big', 'head_long', 'head_round', 'head_small', 'head_square', 'head_wide', 'lips_downturned', 'lips_large', 'lips_medial cleft', 'lips_pouty/full', 'lips_red lipstick', 'lips_thick lower', 'lips_thin', 'lips_thin upper', 'lips_upturned', 'mouth_big/wide', 'mouth_crooked', 'mouth_small', \"neck_Adam's apple\", 'neck_lines', 'neck_tattoos', 'neck_thick', 'nose_bulbous', 'nose_button', 'nose_cleft', 'nose_crooked', 'nose_dorsal hump', 'nose_flared nostrils', 'nose_flat', 'nose_hooked', 'nose_long', 'nose_pointy', 'nose_rounded tip', 'nose_short', 'nose_small', 'nose_small nostrils', 'nose_thin', 'nose_thin bridge', 'nose_upturned', 'nose_v-shaped', 'nose_well-defined tip', 'nose_wide', 'nose_wide bridge', 'nose_wide nostrils', 'nose_wide tip', 'skin_freckles', 'skin_mole', 'skin_pale', 'skin_rough', 'skin_smooth', 'teeth_big', 'teeth_buck', 'teeth_crooked', 'teeth_gap', 'teeth_overbite', 'teeth_small', 'teeth_straight', 'teeth_white']\n",
      "151\n",
      "229\n"
     ]
    }
   ],
   "source": [
    "array_of_possible_sub_features.sort()\n",
    "print(array_of_possible_sub_features)\n",
    "print(len(array_of_possible_sub_features))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nose           23\n",
       "eyes           18\n",
       "hair           16\n",
       "eyebrows       14\n",
       "facial hair    12\n",
       "chin           10\n",
       "lips            9\n",
       "teeth           8\n",
       "ears            8\n",
       "head            6\n",
       "forehead        6\n",
       "skin            5\n",
       "neck            4\n",
       "eyelids         4\n",
       "cheeks          3\n",
       "mouth           3\n",
       "cheekbones      2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = []\n",
    "for fname in array_of_possible_sub_features:\n",
    "    flist = fname.split('_')\n",
    "    counts.append(flist[0])\n",
    "\n",
    "pd.Series(counts).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data = np.empty((len(data), 2), dtype='O')\n",
    "\n",
    "# Iterate through each person, generate a list of 0s, if the feature is present put a 1 in the corresponding index\n",
    "for i in range(len(data)): #iterate through each person\n",
    "    present_features = np.zeros(len(array_of_possible_sub_features))\n",
    "    for j in range(len(data[i][2])):\n",
    "        for k in range(len(data[i][2][j])):\n",
    "            for feature in data[i][2][j]:\n",
    "                present_features[array_of_possible_sub_features.index(feature)] = 1\n",
    "    binary_data[i][0] = data[i][0].strip()\n",
    "    binary_data[i][1] = present_features\n",
    "\n",
    "labels_itemized = pd.DataFrame(binary_data)\n",
    "out_df = pd.DataFrame(labels_itemized.iloc[:, 1].tolist(), index = labels_itemized.iloc[:, 0])\n",
    "out_df = out_df.set_axis(array_of_possible_sub_features, axis=1)\n",
    "\n",
    "#identities = labels_itemized.iloc[:, 0]\n",
    "#out_df.insert(0, 'Identities', identities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cheekbones_high</th>\n",
       "      <th>cheekbones_sharp</th>\n",
       "      <th>cheeks_chubby/full</th>\n",
       "      <th>cheeks_dimples</th>\n",
       "      <th>cheeks_thin/hollow</th>\n",
       "      <th>chin_cleft</th>\n",
       "      <th>chin_crooked</th>\n",
       "      <th>chin_double chin</th>\n",
       "      <th>chin_forward</th>\n",
       "      <th>chin_pointed</th>\n",
       "      <th>...</th>\n",
       "      <th>skin_rough</th>\n",
       "      <th>skin_smooth</th>\n",
       "      <th>teeth_big</th>\n",
       "      <th>teeth_buck</th>\n",
       "      <th>teeth_crooked</th>\n",
       "      <th>teeth_gap</th>\n",
       "      <th>teeth_overbite</th>\n",
       "      <th>teeth_small</th>\n",
       "      <th>teeth_straight</th>\n",
       "      <th>teeth_white</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aamir_khan</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adam_driver</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adam_sandler</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>adele</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aishwarya_rai</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woody_allen</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xi_jinping</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yao_ming</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yo-yo_ma</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zoe_saldana</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>229 rows × 151 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               cheekbones_high  cheekbones_sharp  cheeks_chubby/full   \n",
       "0                                                                      \n",
       "aamir_khan                 0.0               0.0                 0.0  \\\n",
       "adam_driver                0.0               0.0                 0.0   \n",
       "adam_sandler               0.0               0.0                 0.0   \n",
       "adele                      1.0               0.0                 1.0   \n",
       "aishwarya_rai              0.0               0.0                 0.0   \n",
       "...                        ...               ...                 ...   \n",
       "woody_allen                0.0               0.0                 0.0   \n",
       "xi_jinping                 0.0               0.0                 1.0   \n",
       "yao_ming                   1.0               0.0                 0.0   \n",
       "yo-yo_ma                   0.0               0.0                 1.0   \n",
       "zoe_saldana                1.0               0.0                 0.0   \n",
       "\n",
       "               cheeks_dimples  cheeks_thin/hollow  chin_cleft  chin_crooked   \n",
       "0                                                                             \n",
       "aamir_khan                0.0                 0.0         0.0           0.0  \\\n",
       "adam_driver               0.0                 0.0         0.0           1.0   \n",
       "adam_sandler              0.0                 0.0         0.0           0.0   \n",
       "adele                     0.0                 0.0         1.0           0.0   \n",
       "aishwarya_rai             0.0                 0.0         0.0           0.0   \n",
       "...                       ...                 ...         ...           ...   \n",
       "woody_allen               0.0                 0.0         0.0           0.0   \n",
       "xi_jinping                0.0                 0.0         0.0           0.0   \n",
       "yao_ming                  0.0                 0.0         0.0           0.0   \n",
       "yo-yo_ma                  0.0                 0.0         0.0           0.0   \n",
       "zoe_saldana               0.0                 0.0         0.0           0.0   \n",
       "\n",
       "               chin_double chin  chin_forward  chin_pointed  ...  skin_rough   \n",
       "0                                                            ...               \n",
       "aamir_khan                  0.0           0.0           0.0  ...         0.0  \\\n",
       "adam_driver                 0.0           0.0           0.0  ...         0.0   \n",
       "adam_sandler                0.0           0.0           0.0  ...         0.0   \n",
       "adele                       0.0           0.0           0.0  ...         0.0   \n",
       "aishwarya_rai               0.0           0.0           0.0  ...         0.0   \n",
       "...                         ...           ...           ...  ...         ...   \n",
       "woody_allen                 0.0           0.0           0.0  ...         0.0   \n",
       "xi_jinping                  1.0           0.0           0.0  ...         0.0   \n",
       "yao_ming                    0.0           1.0           0.0  ...         0.0   \n",
       "yo-yo_ma                    0.0           0.0           0.0  ...         0.0   \n",
       "zoe_saldana                 0.0           1.0           0.0  ...         0.0   \n",
       "\n",
       "               skin_smooth  teeth_big  teeth_buck  teeth_crooked  teeth_gap   \n",
       "0                                                                             \n",
       "aamir_khan             0.0        0.0         0.0            0.0        0.0  \\\n",
       "adam_driver            0.0        0.0         0.0            0.0        0.0   \n",
       "adam_sandler           0.0        1.0         0.0            0.0        0.0   \n",
       "adele                  0.0        0.0         0.0            0.0        0.0   \n",
       "aishwarya_rai          0.0        0.0         0.0            0.0        0.0   \n",
       "...                    ...        ...         ...            ...        ...   \n",
       "woody_allen            0.0        0.0         0.0            0.0        0.0   \n",
       "xi_jinping             0.0        0.0         0.0            0.0        0.0   \n",
       "yao_ming               0.0        0.0         0.0            0.0        0.0   \n",
       "yo-yo_ma               0.0        0.0         0.0            0.0        0.0   \n",
       "zoe_saldana            1.0        0.0         0.0            0.0        0.0   \n",
       "\n",
       "               teeth_overbite  teeth_small  teeth_straight  teeth_white  \n",
       "0                                                                        \n",
       "aamir_khan                0.0          0.0             0.0          0.0  \n",
       "adam_driver               0.0          0.0             0.0          0.0  \n",
       "adam_sandler              0.0          0.0             0.0          0.0  \n",
       "adele                     0.0          0.0             0.0          0.0  \n",
       "aishwarya_rai             0.0          0.0             0.0          0.0  \n",
       "...                       ...          ...             ...          ...  \n",
       "woody_allen               0.0          0.0             0.0          0.0  \n",
       "xi_jinping                0.0          0.0             0.0          0.0  \n",
       "yao_ming                  0.0          1.0             0.0          0.0  \n",
       "yo-yo_ma                  0.0          0.0             0.0          0.0  \n",
       "zoe_saldana               0.0          0.0             0.0          0.0  \n",
       "\n",
       "[229 rows x 151 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv('./labels/binary_sub_labels_extended.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aamir_khan',\n",
       " 'adam_driver',\n",
       " 'adam_sandler',\n",
       " 'adele',\n",
       " 'aishwarya_rai',\n",
       " 'ajit_pai',\n",
       " 'alec_baldwin',\n",
       " 'alexandria_ocasio-cortez',\n",
       " 'alfred_hitchcock',\n",
       " 'amy_poehler',\n",
       " 'anderson_cooper',\n",
       " 'andy_samberg',\n",
       " 'angela_merkel',\n",
       " 'angelina_jolie',\n",
       " 'anne_hathaway',\n",
       " 'anthony_davis',\n",
       " 'ariana_grande',\n",
       " 'aubrey_plaza',\n",
       " 'ava_gardner',\n",
       " 'awkwafina',\n",
       " 'aziz_ansari',\n",
       " 'barack_obama',\n",
       " 'ben_affleck',\n",
       " 'benedict_cumberbatch',\n",
       " 'benicio_del_toro',\n",
       " 'bernie_sanders',\n",
       " 'betty_white',\n",
       " 'beyonce',\n",
       " 'bill_clinton',\n",
       " 'billie_eilish',\n",
       " 'bob_marley',\n",
       " 'brad_pitt',\n",
       " 'britney_spears',\n",
       " 'bruce_lee',\n",
       " 'bruno_mars',\n",
       " 'cameron_diaz',\n",
       " 'carrie_fisher',\n",
       " 'che_guevara',\n",
       " 'cher',\n",
       " 'chow_yun',\n",
       " 'chris_hemsworth',\n",
       " 'chris_pratt',\n",
       " 'chris_rock',\n",
       " 'chris_tucker',\n",
       " 'christina_aguilera',\n",
       " 'chuck_norris',\n",
       " 'condoleezza_rice',\n",
       " 'conor_mcgregor',\n",
       " 'cristiano_ronaldo',\n",
       " 'cuba_gooding_jr',\n",
       " 'daisy_ridley',\n",
       " 'dalai_lama',\n",
       " 'danny_trejo',\n",
       " 'dave_chappelle',\n",
       " 'david_hasselhoff',\n",
       " 'david_tennant',\n",
       " 'deng_xiaoping',\n",
       " 'denzel_washington',\n",
       " 'diego_maradona',\n",
       " 'dr._phil',\n",
       " 'duterte',\n",
       " 'dwayne_johnson',\n",
       " 'eddie_murphy',\n",
       " 'elizabeth_warren',\n",
       " 'ellen_degeneres',\n",
       " 'ellen_page',\n",
       " 'emilia_clarke',\n",
       " 'eminem',\n",
       " 'emma_watson',\n",
       " 'eva_mendes',\n",
       " 'fergie',\n",
       " 'fidel_castro',\n",
       " 'gal_gadot',\n",
       " 'gandhi',\n",
       " 'george_clooney',\n",
       " 'george_lopez',\n",
       " 'george_rr_martin',\n",
       " 'george_takei',\n",
       " 'george_w._bush',\n",
       " 'halle_berry',\n",
       " 'harrison_ford',\n",
       " 'helena_bonham_carter',\n",
       " 'hillary_clinton',\n",
       " 'hulk_hogan',\n",
       " 'ice_cube',\n",
       " 'jackie_chan',\n",
       " 'james_dean',\n",
       " 'janet_jackson',\n",
       " 'jason_momoa',\n",
       " 'jay_z',\n",
       " 'jennifer_aniston',\n",
       " 'jennifer_garner',\n",
       " 'jennifer_lopez',\n",
       " 'jessica_alba',\n",
       " 'jim_parsons',\n",
       " 'jimi_hendrix',\n",
       " 'john_f._kennedy',\n",
       " 'john_goodman',\n",
       " 'john_krasinski',\n",
       " 'john_mcenroe',\n",
       " 'john_wayne',\n",
       " 'johnny_depp',\n",
       " 'johnny_galecki',\n",
       " 'julia_roberts',\n",
       " 'julian_assange',\n",
       " 'julianne_moore',\n",
       " 'justin_timberlake',\n",
       " 'justin_trudeau',\n",
       " 'kaley_cuoco',\n",
       " 'kanye_west',\n",
       " 'katy_perry',\n",
       " 'keanu_reeves',\n",
       " 'keira_knightley',\n",
       " 'kim_jong_un',\n",
       " 'kirsten_dunst',\n",
       " 'kit_harington',\n",
       " 'kobe_bryant',\n",
       " 'kristen_bell',\n",
       " 'kristen_stewart',\n",
       " 'lady_gaga',\n",
       " 'lebron_james',\n",
       " 'lena_headey',\n",
       " 'leonardo_dicaprio',\n",
       " 'leslie_nielsen',\n",
       " 'lionel_messi',\n",
       " 'lucy_liu',\n",
       " 'luis_suarez',\n",
       " 'madonna',\n",
       " 'maisie_williams',\n",
       " 'mao',\n",
       " 'marc_anthony',\n",
       " 'mariah_carey',\n",
       " 'marilyn_monroe',\n",
       " 'mark_hamill',\n",
       " 'mark_zuckerberg',\n",
       " 'matt_damon',\n",
       " 'matt_smith',\n",
       " 'megan_fox',\n",
       " 'meghan_markle',\n",
       " 'melania_trump',\n",
       " 'melissa_mccarthy',\n",
       " 'meryl_streep',\n",
       " 'michael_bloomberg',\n",
       " 'michael_jackson',\n",
       " 'michelle_obama',\n",
       " 'michelle_rodriguez',\n",
       " 'mike_pence',\n",
       " 'mila_kunis',\n",
       " 'miley_cyrus',\n",
       " 'mitch_mcconnell',\n",
       " 'morgan_freeman',\n",
       " 'nancy_pelosi',\n",
       " 'narendra_modi',\n",
       " 'natalie_dormer',\n",
       " 'neil_patrick_harris',\n",
       " 'neymar',\n",
       " 'nick_offerman',\n",
       " 'nicki_minaj',\n",
       " 'nicole_kidman',\n",
       " 'norah_jones',\n",
       " 'oprah',\n",
       " 'ozzy_osbourne',\n",
       " 'paris_hilton',\n",
       " 'pat_morita',\n",
       " 'patrick_stewart',\n",
       " 'pele',\n",
       " 'penelope_cruz',\n",
       " 'peter_dinklage',\n",
       " 'prince',\n",
       " 'prince_charles',\n",
       " 'priyanka_chopra',\n",
       " 'psy',\n",
       " 'queen_latifah',\n",
       " 'rainn_wilson',\n",
       " 'rashida_jones',\n",
       " 'renee_zellweger',\n",
       " 'richard_nixon',\n",
       " 'rihanna',\n",
       " 'robert_downey_jr.',\n",
       " 'ronald_reagan',\n",
       " 'rosario_dawson',\n",
       " 'ru_paul',\n",
       " 'ryan_reynolds',\n",
       " 'salma_hayek',\n",
       " 'salman_rushdie',\n",
       " 'samuel_l_jackson',\n",
       " 'sandra_bullock',\n",
       " 'sandra_oh',\n",
       " 'sarah_jessica_parker',\n",
       " 'sarah_palin',\n",
       " 'scarlett_johansson',\n",
       " 'seann_william_scott',\n",
       " 'selena',\n",
       " 'selena_gomez',\n",
       " 'shahrukh_khan',\n",
       " 'shakira',\n",
       " 'sharon_osbourne',\n",
       " 'shigeru_miyamoto',\n",
       " 'snoop_dogg',\n",
       " 'sofia_vergara',\n",
       " 'sophie_turner',\n",
       " 'stan_lee',\n",
       " 'steve_buscemi',\n",
       " 'steve_carell',\n",
       " 'steven_tyler',\n",
       " 'steven_yeun',\n",
       " 'taylor_swift',\n",
       " 'ted_cruz',\n",
       " 'terry_crews',\n",
       " 'tilda_swinton',\n",
       " 'tina_fey',\n",
       " 'tina_turner',\n",
       " 'tom_cruise',\n",
       " 'tom_hiddleston',\n",
       " 'tom_holland',\n",
       " 'tracy_morgan',\n",
       " 'vin_diesel',\n",
       " 'vladimir_putin',\n",
       " 'wesley_snipes',\n",
       " 'whitney_houston',\n",
       " 'whoopi_goldberg',\n",
       " 'will_smith',\n",
       " 'willem_dafoe',\n",
       " 'william_shatner',\n",
       " 'woody_allen',\n",
       " 'xi_jinping',\n",
       " 'yao_ming',\n",
       " 'yo-yo_ma',\n",
       " 'zoe_saldana']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df.axes[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cheekbones_high       0.0\n",
       "cheekbones_sharp      0.0\n",
       "cheeks_chubby/full    0.0\n",
       "cheeks_dimples        0.0\n",
       "cheeks_thin/hollow    0.0\n",
       "                     ... \n",
       "teeth_gap             0.0\n",
       "teeth_overbite        0.0\n",
       "teeth_small           0.0\n",
       "teeth_straight        0.0\n",
       "teeth_white           0.0\n",
       "Name: aamir_khan, Length: 151, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'binary_sub_labels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m checkdf \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary_sub_labels.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'binary_sub_labels.csv'"
     ]
    }
   ],
   "source": [
    "checkdf = pd.read_csv('binary_sub_labels.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcheckdf\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkdf' is not defined"
     ]
    }
   ],
   "source": [
    "checkdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "identities = checkdf.axes[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aamir_khan', 'adam_driver', 'adam_sandler', 'adele', 'aishwarya_rai', 'ajit_pai', 'alec_baldwin', 'alexandria_ocasio-cortez', 'alfred_hitchcock', 'amy_poehler', 'anderson_cooper', 'andy_samberg', 'angela_merkel', 'angelina_jolie', 'anne_hathaway', 'anthony_davis', 'ariana_grande', 'aubrey_plaza', 'ava_gardner', 'aziz_ansari', 'barack_obama', 'benedict_cumberbatch', 'benicio_del_toro', 'bernie_sanders', 'betty_white', 'beyonce', 'bill_clinton', 'bob_marley', 'brad_pitt', 'bruce_lee', 'bruno_mars', 'cameron_diaz', 'carrie_fisher', 'che_guevara', 'cher', 'chow_yun', 'chris_hemsworth', 'chris_pratt', 'chris_rock', 'chris_tucker', 'chuck_norris', 'condoleezza_rice', 'conor_mcgregor', 'cristiano_ronaldo', 'cuba_gooding_jr', 'daisy_ridley', 'dalai_lama', 'danny_trejo', 'dave_chappelle', 'david_tennant', 'deng_xiaoping', 'denzel_washington', 'diego_maradona', 'dr._phil', 'duterte', 'eddie_murphy', 'elizabeth_warren', 'ellen_degeneres', 'ellen_page', 'emilia_clarke', 'eminem', 'emma_watson', 'eva_mendes', 'fidel_castro', 'gandhi', 'george_clooney', 'george_lopez', 'george_rr_martin', 'george_takei', 'george_w._bush', 'halle_berry', 'harrison_ford', 'helena_bonham_carter', 'hillary_clinton', 'ice_cube', 'jackie_chan', 'james_dean', 'janet_jackson', 'jason_momoa', 'jay_z', 'jennifer_aniston', 'jennifer_garner', 'jennifer_lopez', 'jessica_alba', 'jim_parsons', 'jimi_hendrix', 'john_f._kennedy', 'john_goodman', 'john_krasinski', 'john_mcenroe', 'john_wayne', 'johnny_depp', 'johnny_galecki', 'julia_roberts', 'julian_assange', 'julianne_moore', 'justin_timberlake', 'justin_trudeau', 'kaley_cuoco', 'kanye_west', 'katy_perry', 'keanu_reeves', 'keira_knightley', 'kim_jong_un', 'kirsten_dunst', 'kit_harington', 'kobe_bryant', 'kristen_bell', 'kristen_stewart', 'lady_gaga', 'lebron_james', 'lena_headey', 'leonardo_dicaprio', 'leslie_nielsen', 'lionel_messi', 'lucy_liu', 'luis_suarez', 'madonna', 'maisie_williams', 'mao', 'marilyn_monroe', 'mark_hamill', 'mark_zuckerberg', 'matt_damon', 'matt_smith', 'megan_fox', 'meghan_markle', 'melania_trump', 'melissa_mccarthy', 'meryl_streep', 'michael_bloomberg', 'michael_jackson', 'michelle_obama', 'michelle_rodriguez', 'mike_pence', 'miley_cyrus', 'mitch_mcconnell', 'morgan_freeman', 'nancy_pelosi', 'narendra_modi', 'natalie_dormer', 'neil_patrick_harris', 'neymar', 'nick_offerman', 'nicki_minaj', 'nicole_kidman', 'oprah', 'patrick_stewart', 'pele', 'penelope_cruz', 'peter_dinklage', 'prince', 'prince_charles', 'priyanka_chopra', 'psy', 'queen_latifah', 'rainn_wilson', 'rashida_jones', 'renee_zellweger', 'richard_nixon', 'rihanna', 'robert_downey_jr.', 'ronald_reagan', 'rosario_dawson', 'ryan_reynolds', 'salma_hayek', 'samuel_l_jackson', 'sandra_bullock', 'sandra_oh', 'sarah_jessica_parker', 'sarah_palin', 'scarlett_johansson', 'seann_william_scott', 'selena_gomez', 'shahrukh_khan', 'shakira', 'snoop_dogg', 'sofia_vergara', 'sophie_turner', 'stan_lee', 'steve_buscemi', 'steve_carell', 'steven_tyler', 'taylor_swift', 'ted_cruz', 'terry_crews', 'tilda_swinton', 'tina_fey', 'tom_cruise', 'tom_hiddleston', 'tom_holland', 'tracy_morgan', 'vin_diesel', 'vladimir_putin', 'wesley_snipes', 'whitney_houston', 'whoopi_goldberg', 'will_smith', 'willem_dafoe', 'william_shatner', 'woody_allen', 'xi_jinping', 'yao_ming', 'zoe_saldana']\n"
     ]
    }
   ],
   "source": [
    "print(identities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cheekbones_high       0.0\n",
       "cheekbones_sharp      0.0\n",
       "cheeks_chubby/full    0.0\n",
       "cheeks_dimples        0.0\n",
       "cheeks_thin/hollow    0.0\n",
       "                     ... \n",
       "teeth_overbite        0.0\n",
       "teeth_small           0.0\n",
       "teeth_straight        0.0\n",
       "teeth_white           0.0\n",
       "upper lip_large       0.0\n",
       "Name: aamir_khan, Length: 150, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkdf.loc['aamir_khan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nose', 'ears', 'eyebrows', 'facial hair', 'eyes', 'eyelids'], ['nose', 'mouth', 'eyes', 'ears', 'facial hair', 'head', 'chin'], ['head', 'nose', 'ears', 'teeth']]\n",
      "[[['nose_well-defined tip'], ['ears_stick out', 'ears_pierced'], ['eyebrows_arched (v-shaped)'], ['facial hair_mustache', 'facial hair_goatee'], ['eyes_bags under eyes'], ['eyelids_puffy']], [['nose_dorsal hump', 'nose_long'], ['mouth_big/wide'], ['eyes_slanted down'], ['ears_stick out', 'ears_low'], ['facial hair_stubble', 'facial hair_goatee'], ['head_long'], ['chin_weak jawline', 'chin_crooked']], [['head_long'], ['nose_dorsal hump'], ['ears_stick out', 'ears_big'], ['teeth_big']]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# iterate through a file and add the labels to a list\n",
    "data = np.array([line.strip().split('||') for line in open('combo-labels-extended.txt')], dtype = list)\n",
    "\n",
    "prominentFeatures = data[:,:-1]\n",
    "\n",
    "prom_features = []\n",
    "sub_features = []\n",
    "\n",
    "# iterate through the list and split the labels into lists\n",
    "for i in range(len(data)):\n",
    "    data[i][1] = data[i][1].split('|')\n",
    "    data[i][1] = [s.strip() for s in data[i][1]]\n",
    "    prom_features.append(data[i][1])\n",
    "    data[i][2] = data[i][2].split('|')\n",
    "    temp_sub_list = []\n",
    "    for j in range(len(data[i][2]) - 1):\n",
    "        data[i][2][j] = data[i][2][j].split(',')\n",
    "        for k in range(len(data[i][2][j])):\n",
    "            data[i][2][j][k] = data[i][1][j].strip() + \"_\" + data[i][2][j][k].strip()\n",
    "        temp_sub_list.append(data[i][2][j])\n",
    "    sub_features.append(temp_sub_list)\n",
    "\n",
    "#np.savetxt('./testlabels.txt', data, delimiter = ',', fmt = '%s')\n",
    "print(prom_features[:3])\n",
    "print(sub_features[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_of_possible_prom_features_new = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    for feature in data[i][1]:\n",
    "        if not feature in array_of_possible_prom_features_new:\n",
    "            array_of_possible_prom_features_new.append(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cheekbones', 'cheeks', 'chin', 'ears', 'eyebrows', 'eyelids', 'eyes', 'facial hair', 'forehead', 'hair', 'head', 'lips', 'mouth', 'neck', 'nose', 'skin', 'teeth', 'upper lip']\n",
      "18\n",
      "229\n"
     ]
    }
   ],
   "source": [
    "array_of_possible_prom_features_new.sort()\n",
    "print(array_of_possible_prom_features_new)\n",
    "print(len(array_of_possible_prom_features_new))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data = np.empty((len(data), 2), dtype='O')\n",
    "\n",
    "# Iterate through each person, generate a list of 0s, if the feature is present put a 1 in the corresponding index\n",
    "for i in range(len(data)): #iterate through each person\n",
    "    present_features = np.zeros(len(array_of_possible_prom_features_new))\n",
    "    for feature in data[i][1]:\n",
    "        present_features[array_of_possible_prom_features_new.index(feature)] = 1\n",
    "    binary_data[i][0] = data[i][0].strip()\n",
    "    binary_data[i][1] = present_features\n",
    "\n",
    "#np.savetxt('binary_labels.txt', binary_data, delimiter = ',', fmt = '%s')\n",
    "#Now the labels are saved \n",
    "#loadtest = np.loadtxt('binary_labels.txt', dtype = 'O', delimiter = ',')\n",
    "# reads in correclty!\n",
    "labels_itemized = pd.DataFrame(binary_data)\n",
    "out_df = pd.DataFrame(labels_itemized.iloc[:, 1].tolist(), index = labels_itemized.iloc[:, 0])\n",
    "out_df = out_df.set_axis(array_of_possible_prom_features_new, axis=1)\n",
    "out_df.to_csv('binary_prom_labels_extended.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "testlist = set(array_of_possible_prom_features) - set(array_of_possible_prom_features_new)\n",
    "print(testlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_of_possible_sub_features_new = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(sub_features[i])):\n",
    "        for sub in sub_features[i][j]:\n",
    "            if not sub in array_of_possible_sub_features_new:\n",
    "                array_of_possible_sub_features_new.append(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cheekbones_high', 'cheekbones_sharp', 'cheeks_chubby/full', 'cheeks_dimples', 'cheeks_thin/hollow', 'chin_cleft', 'chin_crooked', 'chin_double chin', 'chin_forward', 'chin_pointed', 'chin_rounded', 'chin_scar', 'chin_square', 'chin_strong jawline', 'chin_weak jawline', 'ears_big', 'ears_flat', 'ears_high', 'ears_low', 'ears_pierced', 'ears_pointy', 'ears_small', 'ears_stick out', 'eyebrows_arched (v-shaped)', 'eyebrows_bushy', 'eyebrows_curved down', 'eyebrows_far apart', 'eyebrows_flat', 'eyebrows_furrowed', 'eyebrows_light', 'eyebrows_long', 'eyebrows_scar', 'eyebrows_short', 'eyebrows_slanted down', 'eyebrows_thick', 'eyebrows_thin', 'eyebrows_unibrow', 'eyelids_drooping', 'eyelids_hooded', 'eyelids_puffy', 'eyelids_receded', 'eyes_almond', 'eyes_bags under eyes', 'eyes_crows feet', 'eyes_deep-set', 'eyes_glasses', 'eyes_lazy eye', 'eyes_light-colored', 'eyes_long eyelashes', 'eyes_narrow', 'eyes_narrow-set', 'eyes_round', 'eyes_slanted down', 'eyes_slanted up', 'eyes_small', 'eyes_stick out', 'eyes_wide', 'eyes_wide-set', 'eyes_wide-x', 'facial hair_beard', 'facial hair_goatee', 'facial hair_handlebar', 'facial hair_messy', 'facial hair_mustache', 'facial hair_sideburns', 'facial hair_soul patch', 'facial hair_stubble', 'facial hair_thick', 'facial hair_thin', 'facial hair_trimmed', 'facial hair_white', 'forehead_big', 'forehead_narrow', 'forehead_scar', 'forehead_small', 'forehead_wide', 'forehead_wrinkled', 'hair_bald', 'hair_bangs', 'hair_big', 'hair_black', 'hair_blond', 'hair_curly', 'hair_dreads', 'hair_hat', 'hair_long', 'hair_receding hairline', 'hair_red', 'hair_short', 'hair_slicked back', 'hair_white', 'hair_white streaks', 'hair_widows peak', 'head_big', 'head_long', 'head_round', 'head_small', 'head_square', 'head_wide', 'lips_downturned', 'lips_medial cleft', 'lips_pouty/full', 'lips_red lipstick', 'lips_thick lower', 'lips_thin', 'lips_thin upper', 'lips_upturned', 'mouth_big/wide', 'mouth_crooked', 'mouth_small', \"neck_Adam's apple\", 'neck_lines', 'neck_tattoos', 'neck_thick', 'nose_bulbous', 'nose_button', 'nose_cleft', 'nose_crooked', 'nose_dorsal hump', 'nose_flared nostrils', 'nose_flat', 'nose_hooked', 'nose_long', 'nose_pointy', 'nose_rounded tip', 'nose_short', 'nose_small', 'nose_small nostrils', 'nose_thin', 'nose_thin bridge', 'nose_upturned', 'nose_v-shaped', 'nose_well-defined tip', 'nose_wide', 'nose_wide bridge', 'nose_wide nostrils', 'nose_wide tip', 'skin_freckles', 'skin_mole', 'skin_pale', 'skin_rough', 'skin_smooth', 'teeth_big', 'teeth_buck', 'teeth_crooked', 'teeth_gap', 'teeth_overbite', 'teeth_small', 'teeth_straight', 'teeth_white', 'upper lip_large']\n",
      "151\n",
      "229\n"
     ]
    }
   ],
   "source": [
    "array_of_possible_sub_features_new.sort()\n",
    "print(array_of_possible_sub_features_new)\n",
    "print(len(array_of_possible_sub_features_new))\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_data = np.empty((len(data), 2), dtype='O')\n",
    "\n",
    "# Iterate through each person, generate a list of 0s, if the feature is present put a 1 in the corresponding index\n",
    "for i in range(len(data)): #iterate through each person\n",
    "    present_features = np.zeros(len(array_of_possible_sub_features_new))\n",
    "    for j in range(len(data[i][2])):\n",
    "        for k in range(len(data[i][2][j])):\n",
    "            for feature in data[i][2][j]:\n",
    "                present_features[array_of_possible_sub_features_new.index(feature)] = 1\n",
    "    binary_data[i][0] = data[i][0].strip()\n",
    "    binary_data[i][1] = present_features\n",
    "\n",
    "labels_itemized = pd.DataFrame(binary_data)\n",
    "out_df = pd.DataFrame(labels_itemized.iloc[:, 1].tolist(), index = labels_itemized.iloc[:, 0])\n",
    "out_df = out_df.set_axis(array_of_possible_sub_features_new, axis=1)\n",
    "\n",
    "#identities = labels_itemized.iloc[:, 0]\n",
    "#out_df.insert(0, 'Identities', identities)\n",
    "\n",
    "out_df.to_csv('binary_sub_labels_extended.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'facial hair_white'}\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "testlist = set(array_of_possible_sub_features_new) - set(array_of_possible_sub_features)\n",
    "print(testlist)\n",
    "print(len(testlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cheekbones_high       0.0\n",
       "cheekbones_sharp      0.0\n",
       "cheeks_chubby/full    0.0\n",
       "cheeks_dimples        0.0\n",
       "cheeks_thin/hollow    0.0\n",
       "                     ... \n",
       "teeth_overbite        0.0\n",
       "teeth_small           0.0\n",
       "teeth_straight        0.0\n",
       "teeth_white           0.0\n",
       "upper lip_large       0.0\n",
       "Name: aamir_khan, Length: 151, dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df.loc['aamir_khan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cb632aa8d4021609c712c5ba2e650d4f6b36b65bc7c4d2ecc8c6b2392c4f528"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
